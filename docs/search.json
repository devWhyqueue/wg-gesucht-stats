[
  {
    "objectID": "notebooks/proxy_check.html",
    "href": "notebooks/proxy_check.html",
    "title": "Check proxies",
    "section": "",
    "text": "Code\nimport re, os, requests\nimport concurrent.futures as cf\nfrom tqdm import tqdm\n\nFREE_PROXY_LIST_URL = \"https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks4.txt\"\nTARGET_URL = \"https://api.ipify.org\"  # tiny endpoint, returns your IP\nCONNECT_TIMEOUT, READ_TIMEOUT = 3, 5\nMAX_WORKERS = 64  # tune based on ulimit / OS\n\n\n\n\nCode\ndef load_proxies(url=FREE_PROXY_LIST_URL):\n    txt = requests.get(url, timeout=(5,10)).text\n    return list({p for p in re.findall(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}:\\d{2,5}\\b\", txt)})\n\ndef load_proxies_from_file(path: str) -&gt; list[str]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        lines = f.read().splitlines()\n    # keep only host:port patterns\n    return [line.strip() for line in lines if line.strip() and \":\" in line]\n\n\nproxies = load_proxies_from_file(\"out/working_proxies_all.txt\")\nprint(f\"Loaded {len(proxies)} proxies\")\n\n\nLoaded 269 proxies\n\n\n\n\nCode\ndef is_alive(proxy):\n    px = {\"http\": f\"{proxy}\", \"https\": f\"{proxy}\"}\n    try:\n        r = requests.get(TARGET_URL, proxies=px, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT), allow_redirects=False)\n        return proxy if (r.status_code == 200 and r.text.strip()) else None\n    except requests.RequestException:\n        return None\n\n\n\n\nCode\nalive = []\nwith cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n    for res in tqdm(ex.map(is_alive, proxies, chunksize=50), total=len(proxies)):\n        if res: alive.append(res)\n\nprint(f\"Working proxies: {len(alive)}\")\n\n\n100%|██████████| 269/269 [00:12&lt;00:00, 22.28it/s]\n\n\nWorking proxies: 100\n\n\n\n\n\n\n\nCode\nos.makedirs(\"out\", exist_ok=True)\nwith open(\"out/working_proxies.txt\", \"w\") as f:\n    f.write(\"\\n\".join(alive))\n\nprint(\"Saved working proxies to out/working_proxies.txt\")\n\n\nSaved working proxies to out/working_proxies.txt"
  },
  {
    "objectID": "notebooks/flat_ad_details.html",
    "href": "notebooks/flat_ad_details.html",
    "title": "Diving in deeper",
    "section": "",
    "text": "Code\nimport logging\nimport sys\nfrom dataclasses import asdict\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom tqdm.contrib.concurrent import thread_map\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport re, spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\nfrom sentence_transformers import SentenceTransformer\nimport umap\nimport hdbscan\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\nfrom tqdm.auto import tqdm\nimport torch\nfrom wordcloud import WordCloud, STOPWORDS\nfrom IPython.display import display\n\nfrom wggesuchtstats.scraper import get_flat_details\n\n# logging.getLogger(\"backoff\").setLevel(logging.WARNING) \nlogging.getLogger(\"urllib3.connectionpool\").setLevel(logging.WARNING)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\n\ntqdm.pandas()\nload_dotenv()\n\n\nFalse"
  },
  {
    "objectID": "notebooks/flat_ad_details.html#get-the-data",
    "href": "notebooks/flat_ad_details.html#get-the-data",
    "title": "Diving in deeper",
    "section": "Get the data",
    "text": "Get the data\n\n\nCode\n# --- Load & fix URLs ---\ndf = pd.read_csv(\"../data/flat_ads.csv\").reset_index(drop=True)\ndf[\"url\"] = df[\"url\"].apply(\n    lambda u: u if str(u).startswith(\"http\") else f\"https://www.wg-gesucht.de/{str(u).lstrip('/')}\"\n)\n\n\ndf[\"details\"] = thread_map(\n    get_flat_details,\n    df[\"url\"].tolist(),\n    max_workers=64,\n    desc=\"Fetching ads\",\n    chunksize=1,\n)\n\n# --- Expand dataclass fields ---\ndetails_df = df[\"details\"].apply(lambda d: asdict(d) if d is not None else {})\ndf = df.join(pd.json_normalize(details_df)).drop(columns=[\"details\"])\n\n# --- Save ---\ndf.to_csv(\"../data/flat_ads_with_details_new.csv\", index=False, encoding=\"utf-8\")\nlen(df)\n\n\nFetching ads: 100%|██████████| 1766/1766 [07:59&lt;00:00,  3.68it/s]\n\n\n\n\nCode\ndf = pd.read_csv(\"../data/flat_ads_with_details.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nurl\npublished\nrent\nsize\ndistrict\nfemale_inhabitants\nmale_inhabitants\ndiverse_inhabitants\ntotal_inhabitants\nheadline\ndescription\nstreet\nzip_code\navailable_from\navailable_until\nage_min\nage_max\n\n\n\n\n0\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-16T00:00:00\n550\n16\nPrenzlauer Berg\n1\n0\n0\n2\nNUR Berufstätige/Praktikanten: 2-er WG in Pre...\n16qm,3m Zimmerhöhe, Südosten, 4.OG, sehr hell ...\nDanziger Str\n10407.0\n2025-09-01\n2025-12-01\n49.0\n49.0\n\n\n1\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-16T00:00:00\n999\n20\nPrenzlauer Berg\n1\n1\n0\n4\nURBANELITE.COM // Keine Kaution! (keine Absich...\n- &gt; WWW. URBANELITE.COM \\n\\n WIR ARE: \\n Wir s...\nSchönhauser Allee\n10439.0\n2025-08-16\nNaN\nNaN\nNaN\n\n\n2\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-16T00:00:00\n650\n20\nPrenzlauer Berg\n0\n1\n0\n2\nBright Room for Sublet in Prenzlauer Berg (6 M...\nIch biete meine ca. 20 m2 Zimmer in einer reno...\nDunckerstraße\n10439.0\n2025-10-01\n2026-03-31\n35.0\n40.0\n\n\n3\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-15T00:00:00\n750\n16\nFriedrichshain\n0\n1\n0\n2\nGemütliches Sonnenzimmer in Fshain\nMieten Mein schön sonniges Zimmer – 2 Monate (...\nBosestraße 6A\n10245.0\n2025-09-01\n2026-04-01\n30.0\n36.0\n\n\n4\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-07-14T00:00:00\n900\n20\nKreuzberg\n1\n0\n0\n2\nWG room 20m2 Mehringdamm\nZimmer verfügbar ab September. \\n Letztendlich...\nGroßbeerenstraße 28\n10965.0\n2025-08-15\nNaN\n28.0\n28.0\n\n\n\n\n\n\n\n\n\nCode\ndf = df[df[\"headline\"].notna()] \nlen(df)\n\n\n3747"
  },
  {
    "objectID": "notebooks/flat_ad_details.html#age",
    "href": "notebooks/flat_ad_details.html#age",
    "title": "Diving in deeper",
    "section": "Age",
    "text": "Age\n\n\nCode\ndef get_stats(series):\n    return {\n        \"min\": series.min(),\n        \"max\": series.max(),\n        \"mean\": series.mean(),\n        \"median\": series.median()\n    }\n\nage_min_stats = get_stats(df[\"age_min\"].dropna())\nage_max_stats = get_stats(df[\"age_max\"].dropna())\n\n# Histogram with legend showing stats\nplt.figure(figsize=(8,5))\nsns.histplot(df[\"age_min\"], color=\"blue\", kde=True, alpha=0.5, label=\"age_min\")\nsns.histplot(df[\"age_max\"], color=\"red\", kde=True, alpha=0.5, label=\"age_max\")\n\n# Add stats to legend\nlegend_text = [\n    f\"age_min: min:{age_min_stats['min']}, max:{age_min_stats['max']}, \"\n    f\"mean:{age_min_stats['mean']:.1f}, median:{age_min_stats['median']}\",\n    f\"age_max: min:{age_max_stats['min']}, max:{age_max_stats['max']}, \"\n    f\"mean:{age_max_stats['mean']:.1f}, median:{age_max_stats['median']}\"\n]\n\nplt.legend(legend_text)\nplt.title(\"Min/max age of inhabitants\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_cleaned = df.dropna(subset=[\"age_min\",\"age_max\"])\nX = df_cleaned[[\"age_min\",\"age_max\"]]\n\n# --- Fit KMeans ---\nbest_k = 3  # &lt;- set this after elbow method\nkmeans = KMeans(n_clusters=best_k, random_state=42, n_init=\"auto\")\ndf_cleaned[\"cluster\"] = kmeans.fit_predict(X)\n\ncluster_names = {\n    0: \"Students/Young Professionals\",\n    1: \"Cross-generational\",\n    2: \"Working-age\"\n}\n\n# Map numeric cluster labels to names\ndf_cleaned[\"cluster_name\"] = df_cleaned[\"cluster\"].map(cluster_names)\n\n# --- Cluster sizes ---\ncluster_counts = df_cleaned[\"cluster\"].value_counts(normalize=True).sort_index() * 100\n\n# --- Scatterplot ---\nplt.figure(figsize=(7,6))\nsns.scatterplot(data=df_cleaned, x=\"age_min\", y=\"age_max\", hue=\"cluster_name\", palette=\"Set2\", alpha=0.6)\n\n# cluster centers\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:,0], centers[:,1], c=\"black\", s=150, marker=\"X\", label=\"Centers\")\n\n# annotate percentages\nfor idx, (x, y) in enumerate(centers):\n    pct = cluster_counts[idx]\n    plt.text(x, y+1, f\"{pct:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\", color=\"black\")\n\nplt.plot([0,60],[0,60], color=\"gray\", linestyle=\"--\")\nplt.xlabel(\"age_min\")\nplt.ylabel(\"age_max\")\nplt.title(f\"KMeans Clusters of WG Ads (k={best_k})\")\nplt.legend()\nplt.show()\n\n\nC:\\Users\\Yannik\\AppData\\Local\\Temp\\ipykernel_12704\\2865640186.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cleaned[\"cluster\"] = kmeans.fit_predict(X)\nC:\\Users\\Yannik\\AppData\\Local\\Temp\\ipykernel_12704\\2865640186.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cleaned[\"cluster_name\"] = df_cleaned[\"cluster\"].map(cluster_names)"
  },
  {
    "objectID": "notebooks/flat_ad_details.html#nlp",
    "href": "notebooks/flat_ad_details.html#nlp",
    "title": "Diving in deeper",
    "section": "NLP",
    "text": "NLP\n\n\nCode\ndf[\"text\"] = (df[\"headline\"].fillna(\"\") + \" \" + df[\"description\"].fillna(\"\")).str.strip()\ndf_nlp = df[df[\"text\"].str.len() &gt; 0].copy()\n\n\n\n\nCode\n\n\nnlp = spacy.load(\"de_core_news_sm\", disable=[\"parser\"])\nSTOP_EXTRA = set(line.strip() for line in open(\"../data/stop_words.txt\", encoding=\"utf-8\"))\n\ndef clean_txt(s: str) -&gt; str:\n    s = s.lower()\n    s = re.sub(r\"\\s+\", \" \", s)\n    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n    s = re.sub(r\"[\\d_]+\", \" \", s)\n    return s.strip()\n\ndf_nlp[\"text_clean\"] = df_nlp[\"text\"].progress_apply(clean_txt)\ntexts = df_nlp[\"text_clean\"].tolist()\nlemmas = []\nfor doc in tqdm(nlp.pipe(texts, batch_size=64), total=len(texts), desc=\"lemmatizing\"):\n    toks = [\n        t.lemma_.lower()\n        for t in doc\n        if t.is_alpha and not t.is_stop and len(t.lemma_) &gt;= 3 and t.lemma_.lower() not in STOP_EXTRA\n    ]\n    lemmas.append(\" \".join(toks))\n\ndf_nlp[\"text_lem\"] = lemmas\n\n\n100%|██████████| 3747/3747 [00:00&lt;00:00, 3882.27it/s]\nlemmatizing: 100%|██████████| 3747/3747 [04:03&lt;00:00, 15.38it/s]\n\n\n\n\nCode\ndef top_tfidf_terms(texts, n=20, ngram=(1,1), min_df=5, max_df=0.6):\n    v = TfidfVectorizer(ngram_range=ngram, min_df=min_df, max_df=max_df)\n    X = v.fit_transform(texts)\n    mean_scores = np.asarray(X.mean(axis=0)).ravel()\n    idx = mean_scores.argsort()[::-1][:n]\n    return pd.DataFrame({\"term\": np.array(v.get_feature_names_out())[idx],\n                         \"tfidf\": mean_scores[idx]})\n\ntop_uni = top_tfidf_terms(df_nlp[\"text_lem\"], n=25, ngram=(1,1))\ntop_bi  = top_tfidf_terms(df_nlp[\"text_lem\"], n=25, ngram=(2,2))\nprint(top_uni.head(10)); print(top_bi.head(10))\n\n\n       term     tfidf\n0     ruhig  0.028846\n1     schön  0.027227\n2     küche  0.022431\n3    balkon  0.021267\n4    liegen  0.021082\n5    direkt  0.020641\n6    suchen  0.020607\n7    person  0.019227\n8  befinden  0.018281\n9  entfernt  0.017532\n                        term     tfidf\n0            prenzlauer berg  0.012642\n1            fully furnished  0.007785\n2                  küche bad  0.007551\n3             voll möblieren  0.007408\n4  öffentlich verkehrsmittel  0.007113\n5          voll ausgestattet  0.007092\n6          bett schreibtisch  0.007062\n7         ausgestattet küche  0.006857\n8           tempelhofer feld  0.006396\n9                hoch decken  0.006232\n\n\n\n\nCode\nstopword_list = STOPWORDS.union(STOP_EXTRA)\n\n# TF-IDF → mean weight per term\nv = TfidfVectorizer(ngram_range=(1,1), min_df=5, max_df=0.6)\nX = v.fit_transform(df_nlp[\"text_lem\"])\nweights = np.asarray(X.mean(axis=0)).ravel()\nterms = v.get_feature_names_out()\nfreq = {t: float(w) for t, w in zip(terms, weights)}\n\nwc = WordCloud(width=1400, height=900, background_color=\"white\", stopwords=stopword_list)\nimg = wc.generate_from_frequencies(freq)\n\nplt.figure(figsize=(12,8))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"WG Ads — TF-IDF Word Cloud (unigrams)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef lsa_topics(texts, n_topics=8, n_terms=10, min_df=5, max_df=0.6, ngram=(1,2)):\n    v = TfidfVectorizer(ngram_range=ngram, min_df=min_df, max_df=max_df)\n    X = v.fit_transform(texts)\n    svd = TruncatedSVD(n_components=n_topics, random_state=42)\n    W = svd.fit_transform(X)\n    H = svd.components_\n    terms = v.get_feature_names_out()\n    topics = []\n    for k in range(n_topics):\n        idx = np.argsort(H[k])[::-1][:n_terms]\n        topics.append({\"topic\": k, \"terms\": \", \".join(terms[i] for i in idx)})\n    doc_topics = W.argmax(axis=1)\n    return pd.DataFrame(topics), pd.Series(doc_topics, name=\"topic\")\n\ntopics_df, df_nlp[\"topic_lsa\"] = lsa_topics(df_nlp[\"text_lem\"], n_topics=6, n_terms=12)\nprint(topics_df)\n\n\n   topic                                              terms\n0      0  ruhig, schön, küche, from, direkt, suchen, per...\n1      1  our, your, flat, from, can, community, have, t...\n2      2  community, our, hinaus, netzwerk, network, mor...\n3      3  mal, wichtig, leben, that, mensch, but, kochen...\n4      4  prenzlauer, berg, prenzlauer berg, allee, schö...\n5      5  august, neukölln, september, kreuzberg, schön,...\n\n\n\n\nCode\nmodel = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\nemb = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n\num = umap.UMAP(n_neighbors=15, min_dist=0.05, n_components=2, random_state=42)\nxy = um.fit_transform(emb)\n\ncl = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=10, metric=\"euclidean\").fit(emb)\ndf_nlp[\"cluster_hdb\"] = cl.labels_  # -1 = noise\n\n# Inspect clusters\nprint(df_nlp[\"cluster_hdb\"].value_counts().head(10))\n\n\n2025-08-19 14:25:55,237 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n2025-08-19 14:25:55,239 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n\n\nBatches: 100%|██████████| 118/118 [04:04&lt;00:00,  2.07s/it]\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n\n\ncluster_hdb\n 0    2632\n-1    1075\n 1      40\nName: count, dtype: int64\n\n\n\n\nCode\ndef cluster_keywords(texts, labels, topn=12):\n    out = []\n    for lab in sorted(set(labels)):\n        if lab == -1: continue\n        subset = [t for t,l in zip(texts, labels) if l == lab]\n        tt = top_tfidf_terms(subset, n=topn, ngram=(1,2))\n        out.append((lab, \", \".join(tt.term.tolist())))\n    return pd.DataFrame(out, columns=[\"cluster\",\"keywords\"])\n    \nprint(cluster_keywords(df_nlp[\"text_lem\"].tolist(), df_nlp[\"cluster_hdb\"].tolist()))\n\n\n   cluster                                           keywords\n0        0  ruhig, schön, küche, liegen, balkon, direkt, s...\n1        1  friedrichshain, kreuzberg, friedrichshain idea...\n\n\n\n\nCode\nMODEL = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\ntorch.set_num_threads(3)\ndevice = 0 if torch.cuda.is_available() else -1\n\ntok = AutoTokenizer.from_pretrained(MODEL)\nmdl = AutoModelForSequenceClassification.from_pretrained(MODEL)\npipe = TextClassificationPipeline(model=mdl, tokenizer=tok, device=device, return_all_scores=True)\n\ndef fast_sentiment(texts, batch_size=64, max_len=256, desc=\"Sentiment\"):\n    labels, scores = [], []\n    n_batches = (len(texts) + batch_size - 1) // batch_size\n    for i in tqdm(range(n_batches), desc=desc):\n        batch = texts[i*batch_size:(i+1)*batch_size]\n        out = pipe(batch, truncation=True, max_length=max_len)\n        for row in out:\n            best = max(row, key=lambda x: x[\"score\"])\n            labels.append(best[\"label\"].lower())\n            scores.append(float(best[\"score\"]))\n    return labels, scores\n\ndf_nlp[\"sentiment\"], df_nlp[\"sentiment_score\"] = fast_sentiment(df_nlp[\"text\"].tolist())\n\n\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n\n\n2025-08-19 16:08:27,175 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n\n\nDevice set to use cpu\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nSentiment: 100%|██████████| 59/59 [21:20&lt;00:00, 21.70s/it]\n\n\n\n\nCode\ndf_nlp[\"sentiment\"].value_counts(normalize=True).mul(100).round(1)\n\n\nsentiment\npositive    82.7\nnegative    16.2\nneutral      1.1\nName: proportion, dtype: float64\n\n\n\n\nCode\npd.set_option(\"display.max_colwidth\", None)  # no truncation\n\n# get most positive (highest score where label=positive)\npos = df_nlp[df_nlp[\"sentiment\"]==\"positive\"].sort_values(\"sentiment_score\", ascending=False).head(5)\n\n# get most negative (highest score where label=negative)\nneg = df_nlp[df_nlp[\"sentiment\"]==\"negative\"].sort_values(\"sentiment_score\", ascending=False).head(5)\n\n# select just relevant fields\ncompare = pd.DataFrame({\n    \"Most Positive\": pos[\"text\"].values,\n    \"Most Negative\": neg[\"text\"].values\n})\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nMost Positive\nMost Negative\n\n\n\n\n0\nschönes WG- Zimmer in Berlin Prenzlauerberg, befristet schönes, helles Zimmer in Berlin Prenzlauer Berg mit Balkon\nWilmersdorf ist ruhig und angesagt! Das Zimmer ist möbliert. Die öffentlichen Verkehrsmittel sind fussläufig in 3 Minuten erreichbar. Die Supermärkte in 5 Minuten.\n\n\n1\nRoom in Prenzlauer Berg Very nice and bright room, king size bed, plants, table and balcony\\nVery well located in the lively neighborhood of Prenzlauer Berg\\nKitchen\nPrivatzimmer 24 qm in Tempelhof nahe Ringbahn an eine Frau im Notfall auch früher möglich aber auch etwas länger Zeit punkt ist auch flexibel Das Zimmer ist groß, hell und sehr ruhig es gibt 2 Zimmer und die Wohnung ist im 2. OG\\nIn dem sehr Grünen Bezirk ist es auch an den Wochenenden sehr leicht möglich einen Einkauf zu erledigen, auch ist die Nähe zum Ring Bahnhof eine Garantie leicht überall in Berlin hin zu kommen\\nIch bin ein Mann und 60 Jahre jung, durch eine Krankheit ( die nicht ansteckend ist ) , bin ich meist zuhause, versuche aber auch dennoch etwas zu finden das mich zeitweise beschäftigt. Ich suche wegen der doch sehr vielen schlechten Erfahrungen mit Männern, auch wenn es anders besprochen wurde lieber eine Weibliche Mitbewohnerin.\\nKontakt ist jederzeit möglich über die Nachrichtenfunktion hier. Da es ja immer erst einmal ein Versuch des Zusammen Lebens ist kann auch etwas langfristiges daraus entstehen wenn man sich versteht\n\n\n2\nschönes helles Zimmer schönes helles Zimmer in Berlin Reinickendorf mit guter Infrastruktur.Die Wohnung liegt im Norden von Betlin.U Bahn Rathaus Reikickendorf ist 10 Minuten Fussläufig entfernt.\nUntermiete 12.09.-27.10.25 in Berlin Weißensee 20qm mit 2 fenster und großem bett. \\n Schriebtisch und sessel und viele pflanzen sind auch da - ich raume naturlich alles vorher aus und mach das fur dich nochmal schick - gleiches gilt für bad und küche\\nRuhes und grunes weisensee \\n M2 - S prenzlauer Allee (5min) \\n M2 - Alexanderplatz (20min) \\n\\n - kannst aber auch ein rad von mir fur die zeit haben\\nMein mitbewohner ist ruhig und entspannt und korrekt. Er hat auch ein Veto recht, da er die zeit da ist.\\nZeit und kohle ist verhandlungssache\n\n\n3\nschön wg ab 15.august Hey! \\n Ich verlasse meine schöne kleine Wohnung, um nach Frankreich zu ziehen und zu studieren. \\n 🔹Das Zimmer ist ca. 20m2, mit einem Verbindungsbalkon zum anderen Zimmer, Isa's, die immer sehr positiv und großartig ist, um ein Glas Wein und einen guten Chat mit zu teilen. \\n Es gibt auch Mira, die gerne Musik hören und machen und immer ein großes Lachen ist. \\n 🔺Das Zimmer ist ab 15. August verfügbar! 🔺 \\n Die Wohnung befindet sich direkt im Zentrum von Berlin, in der Nähe aller Annehmlichkeiten! \\n 🔹Nur 8 Gehminuten vom Alexanderplatz und 5 Minuten vom U-Bahnhof jannowitzbrücke entfernt. \\n 🔹Several Supermärkte direkt auf der Straße. \\n 🔸Senden Sie mir eine Einführung zu sich selbst, und wir arrangieren einen Besuch zusammen! \\n\\n Freuen Sie sich auf eine Anhörung von Ihnen (:\\n \\n\\n Heyy there 🌞! \\n I'm leaving my lovely little flat-share to move back to France and study. \\n 🔹The room is about 20m2, with a connecting balcony to the other room, Isa's, who's always very positive and great to share a glass of wine and a good chat with. \\n There's also mira, who loves listening and making music, and is always a great laugh. \\n 🔺The room is available from August 15! 🔺 \\n The apartment is located right in the center of Berlin, close to all amenities! \\n 🔹Just an 8-minute walk from Alexanderplatz and 5 minutes from the jannowitzbrücke subway station. \\n 🔹Several supermarkets right down the street. \\n 🔸Send me an introduction to yourself, and we'll arrange a visit together! \\n\\n Looking forward to hearing from you (:\\n \\n\\nText automatisch übersetzt -\\nOriginal anzeigen\\nÜbersetzung anzeigen\nWohnen nähe Volkspark Humboldthain / All-inclusive Zimmer: \\r\\n - Boxspringbett \\r\\n - großer Kleiderschrank \\r\\n - Smart-TV \\r\\n - Schreibtisch \\r\\n - Balkon \\n\\r\\n Küche: \\r\\n - Einbauküche mit Herd und Ofen \\r\\n - Geschirrspüler \\r\\n - Waschmaschine \\r\\n - großer Kühlschrank mit Gefrierkombination \\r\\n - Toaster, Kaffeemaschine etc. \\n\\r\\n Badezimmer: \\r\\n - Duschwanne \\r\\n - WC \\r\\n - Wäschespinne \\r\\n - Bügelbrett & Bügeleisen \\n\\n\\n\\r\\n Room: \\r\\n - Box spring bed \\r\\n - large closet \\r\\n - smart TV \\r\\n - desk \\r\\n - balcony \\n\\r\\n Kitchen: \\r\\n - Fitted kitchen with stove and oven \\r\\n - dishwasher \\r\\n - washing machine \\r\\n - large fridge with freezer combination \\r\\n - Toaster, coffee machine etc. \\n\\r\\n Bathroom: \\r\\n - Shower tray \\r\\n - WC \\r\\n - rotary dryer \\r\\n - Ironing board & iron\\nDas Objekt liegt aufstrebenden Ortsteil Gesundbrunnen, profitieren vom Ortsteil Mitte. Die Lage ist durch den Flair eines lebendigen und einfachen Anwohner geprägt, das eine angenehme Wohnatmosphäre bietet. \\n Das ist nicht gut. Sie von einer guten Nahverkehrsanbindung: Die U-Bahnlinie 9 ist fußläufig innerhalb von ca. 8 erreichbar, was eine schnelle Verbindung ins Stadtzentrum und zu weiteren Teilen ermöglicht. Einrichtungen sich mehrere Bushaltestellen in der Nähe, mit den Linien M27, 147 und 255 genießen Sie bequem in allen Teilen der Hauptstadt. \\n In der näheren Umgebung finden Sie zahlreiche Einkaufsmöglichkeiten, darunter Supermarktketten mit guten Parkmöglichkeiten, die den täglichen Bedürfnissen anpassen. vielfältige Restaurants, Cafés und Dienstleistungsangebote in der Nähe vorhanden. \\n Für Erholung und Freizeit bieten sich die nahegelegenen Flächen Grün und Parks an: Der Volkspark Humboldthain mit seinem großen Parkgelände, den Wasserflächen und dem bekannten Flakturm ist nur einige Minuten entfernt und lädt zu Spaziergänge, Jogging oder Picknicks ein. Auch der Schönhauser Park sowie der Gleisdreieckpark sind gut erreichbar und bieten vielfältige Möglichkeiten für Freizeitaktivitäten im Grünen. \\n Die Nähe zu diesen Parks macht die Lage besonders lebenswert und bietet eine schöne Balance zwischen urbanem Leben und Natur. \\n\\n\\n\\n Das Apartment befindet sich im ankommenden Stadtteil Gesundbrunnen, direkt neben dem Stadtteil Mitte. Die Lage zeichnet sich durch das Flair einer lebhaften und dennoch ruhigen Wohngegend aus, die eine angenehme Wohnatmosphäre bietet. \\n Gleichzeitig profitieren Sie von ausgezeichneten lokalen Verkehrsverbindungen: Die U-Bahnlinie 9 ist zu Fuß von ca. 8 Minuten, eine schnelle Verbindung zum Stadtzentrum und anderen Teilen der Stadt. Es gibt auch mehrere Bushaltestellen in der Nähe, mit den Linien M27, 147 und 255 bietet einfachen Zugang zu allen Teilen der Hauptstadt. \\n In der unmittelbaren Umgebung finden Sie zahlreiche Einkaufsmöglichkeiten, darunter Supermarktketten mit guten Parkplätzen, die den täglichen Einkauf bequem machen. Es gibt auch verschiedene Restaurants, Cafés und Service-Outlets in der Nähe. \\n Die nahegelegenen Grünflächen und Parks sind ideal für Erholung und Freizeit: Der Volkspark Humboldthain mit seiner großen Parkanlage, Wasseranlagen und der berühmte Flakturm ist nur wenige Minuten entfernt und ist ideal für Spaziergänge, Joggen oder Picknicks. Der Schönhauser Park und der Gleisdreieckpark sind ebenfalls leicht zu erreichen und bieten eine breite Palette an Freizeitaktivitäten auf dem Land. \\n Die Nähe zu diesen Parks macht die Lage besonders lebendig und bietet eine schöne Balance zwischen urbanem Leben und Natur.\\n \\n\\n Das Objekt liegt im aufstrebenden Ortsteil Gesundbrunnen, unmittelbar benachbart zum Ortsteil Mitte. Die Lage ist durch den Flair eines lebendigen und dennoch ruhigen Anwohnergebietes geprägt, das eine angenehme Wohnatmosphäre bietet. \\r\\n Gleichzeitig profitieren Sie von einer ausgesprochen guten Nahverkehrsanbindung: Die U-Bahnlinie 9 ist fußläufig innerhalb von ca. 8 Minuten erreichbar, was eine schnelle Verbindung ins Stadtzentrum und zu weiteren Stadtteilen ermöglicht. Zudem befinden sich mehrere Bushaltestellen in der Nähe, mit den Linien M27, 147 und 255 gelangen Sie bequem in alle Teile der Hauptstadt. \\r\\n In der näheren Umgebung finden Sie zahlreiche Einkaufsmöglichkeiten, darunter Supermarktketten mit guten Parkmöglichkeiten, die den täglichen Einkauf bequem machen. Zudem sind diverse Restaurants, Cafés und Dienstleistungsangebote in der Nähe vorhanden. \\r\\n Für Erholung und Freizeit bieten sich die nahegelegenen Grünflächen und Parks an: Der Volkspark Humboldthain mit seinem großen Parkgelände, den Wasserflächen und dem bekannten Flakturm ist nur wenige Minuten entfernt und lädt zu Spaziergängen, Jogging oder Picknicks ein. Auch der Schönhauser Park sowie der Gleisdreieckpark sind gut erreichbar und bieten vielfältige Möglichkeiten für Freizeitaktivitäten im Grünen. \\r\\n Die Nähe zu diesen Parks macht die Lage besonders lebenswert und bietet eine schöne Balance zwischen urbanem Leben und Natur. \\n\\n\\n\\r\\n The apartment is located in the up-and-coming district of Gesundbrunnen, directly adjacent to the Mitte district. The location is characterized by the flair of a lively yet quiet residential area that offers a pleasant living atmosphere. \\r\\n At the same time, you benefit from excellent local transport connections: The subway line 9 is within walking distance of approx. 8 minutes, providing a quick connection to the city center and other parts of the city. There are also several bus stops nearby, with the M27, 147 and 255 lines providing easy access to all parts of the capital. \\r\\n In the immediate area, you will find numerous shopping facilities, including supermarket chains with good parking facilities, which make daily shopping convenient. There are also various restaurants, cafés and service outlets nearby. \\r\\n The nearby green spaces and parks are ideal for recreation and leisure: Volkspark Humboldthain with its large park area, water features and the famous flak tower is just a few minutes away and is ideal for walks, jogging or picnics. Schönhauser Park and Gleisdreieckpark are also within easy reach and offer a wide range of leisure activities in the countryside. \\r\\n The proximity to these parks makes the location particularly liveable and offers a nice balance between urban life and nature.\\n \\n\\nText automatisch übersetzt -\\nOriginal anzeigen\\nÜbersetzung anzeigen\\nDas Zimmer befindet sich in einer ca. 63 m² großen Wohnung im 2. OG eines modernen Neubaus in Wedding. \\r\\n Dein neues Zuhause ist vollständig ausgestattet und wird all-inclusive vermietet. \\n\\r\\n Der Mietpreis setzt sich aus den Kosten für die Wohnräume und die Möblierung zusammen. Des Weiteren sind die Kosten für alle Neben-, Heiz- und Warmwasserkosten sowie die Stromkosten enthalten. Ein 200.000er Internetvertrag ist bereits abgeschlossen und inklusive. Wir übernehmen außerdem die Beitragszahlungen an die GEZ. \\n\\r\\n Gerne stehen wir Ihnen, nach Vertragsabschluss für alle Probleme 10 Stunden Wochentags zur Verfügung. \\n\\n\\n\\r\\n The room is located in an approx. 63 m² apartment on the 2nd floor of a modern new building in Wedding. \\r\\n Your new home is fully furnished and is rented all-inclusive. \\n\\r\\n The rent is made up of the costs for the living space and furnishings. It also includes all utilities, heating and hot water costs as well as electricity. A 200,000 internet contract has already been concluded and is included. We also pay the GEZ contributions. \\n\\r\\n We will be happy to help you with any problems 10 hours a week after the contract has been signed.\\nDie Mietdauer beläuft sich auf 6 bis 11 Monate. \\n\\r\\n Alle Angaben in diesem Exposé (Objektbeschreibung, Abmessungen, Preisangaben etc.) beruhen auf Angaben des Eigentümers und erfolgen ohne Gewähr. \\n\\r\\n Diese Wohnung wird nur für den vorübergehenden Gebrauch zur Verfügung gestellt. \\n\\n\\n\\r\\n The rental period is between 6 and 11 months. \\n\\r\\n All information in this exposé (property description, dimensions, price details, etc.) is based on information provided by the owner and is given without guarantee. \\n\\r\\n This apartment is only made available for temporary use.\n\n\n4\nschönes zimmer NUR FÜR FRAUEN Linda habitación con jardín . \\r\\n Schönes Zimmer mit Große garten direkt am Frankfurter alle Boxagener Kiez, Kaffes Restaurants, Einkauf Möglichkeit ganz in der nähe\nZimmer in Lankwitz (August/September) Da ich im August & September nicht in Berlin sein werde, vermiete ich mein Zimmer unter. Es ist möbliert (Bett, Schreibtisch, Stuhl, Schrank, WLAN), Bad & Küche werden mit einer weiteren Frau geteilt.\n\n\n\n\n\n\n\n\n\nCode\ndef quick_ner_counts(texts, topn=20):\n    cats = []\n    for doc in nlp.pipe(texts, batch_size=64):\n        for e in doc.ents:\n            if e.label_ in {\"LOC\",\"PER\",\"ORG\",\"MISC\",\"DATE\",\"MONEY\"}:\n                cats.append((e.text.lower(), e.label_))\n    s = pd.DataFrame(cats, columns=[\"ent\",\"label\"]).value_counts().reset_index(name=\"count\")\n    return s.groupby(\"label\").head(topn)\n    \nner_top = quick_ner_counts(df_nlp[\"text\"].head(5000))  # sample for speed\nprint(ner_top.head(15))"
  },
  {
    "objectID": "notebooks/flat_ads.html",
    "href": "notebooks/flat_ads.html",
    "title": "WG Gesucht Stats",
    "section": "",
    "text": "last updated 17.08.2025\nSome statistics about the market in Berlin.\nCode\nimport logging\nimport sys\n\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as pe\nimport numpy as np\nimport geopandas as gpd\n\nfrom wggesuchtstats.scraper import find_shared_flats\nfrom wggesuchtstats.models import to_csv\n\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\n\nload_dotenv()\n\n\nTrue"
  },
  {
    "objectID": "notebooks/flat_ads.html#get-the-data",
    "href": "notebooks/flat_ads.html#get-the-data",
    "title": "WG Gesucht Stats",
    "section": "Get the data",
    "text": "Get the data\n\n\nCode\nflat_ads = find_shared_flats()\nto_csv(flat_ads, \"data/flat_ads.csv\")\nlen(flat_ads)"
  },
  {
    "objectID": "notebooks/flat_ads.html#clean-it",
    "href": "notebooks/flat_ads.html#clean-it",
    "title": "WG Gesucht Stats",
    "section": "Clean it",
    "text": "Clean it\n\n\nCode\n# Load CSV\ndf = pd.read_csv(\"../data/flat_ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nurl\npublished\nrent\nsize\ndistrict\nfemale_inhabitants\nmale_inhabitants\ndiverse_inhabitants\ntotal_inhabitants\n\n\n\n\n0\nwg-zimmer-in-Berlin-Prenzlauer-Berg.6103697.html\n2025-08-16T00:00:00\n550\n16\nPrenzlauer Berg\n1\n0\n0\n2\n\n\n1\nwg-zimmer-in-Berlin-Prenzlauer-Berg.9961003.html\n2025-08-16T00:00:00\n999\n20\nPrenzlauer Berg\n1\n1\n0\n4\n\n\n2\nwg-zimmer-in-Berlin-Prenzlauer-Berg.12248079.html\n2025-08-16T00:00:00\n650\n20\nPrenzlauer Berg\n0\n1\n0\n2\n\n\n3\nwg-zimmer-in-Berlin-Friedrichshain.12230915.html\n2025-08-15T00:00:00\n750\n16\nFriedrichshain\n0\n1\n0\n2\n\n\n4\nwg-zimmer-in-Berlin-Kreuzberg.12158342.html\n2025-07-14T00:00:00\n900\n20\nKreuzberg\n1\n0\n0\n2"
  },
  {
    "objectID": "notebooks/flat_ads.html#data-time-range",
    "href": "notebooks/flat_ads.html#data-time-range",
    "title": "WG Gesucht Stats",
    "section": "Data Time Range",
    "text": "Data Time Range\n\n\nCode\n# ensure datetime\ndf[\"published\"] = pd.to_datetime(df[\"published\"])\n\nmonthly_counts = (\n    df.groupby(df[\"published\"].dt.to_period(\"M\"))\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values(\"published\")   # sort Periods\n)\n\n# convert Period to timestamp AFTER sorting\nmonthly_counts[\"published\"] = monthly_counts[\"published\"].dt.to_timestamp()\n\nmonthly_counts\n\n\n\n\n\n\n\n\n\npublished\ncount\n\n\n\n\n0\n2024-06-01\n1\n\n\n1\n2024-12-01\n1\n\n\n2\n2025-01-01\n1\n\n\n3\n2025-02-01\n3\n\n\n4\n2025-03-01\n7\n\n\n5\n2025-04-01\n9\n\n\n6\n2025-05-01\n31\n\n\n7\n2025-06-01\n116\n\n\n8\n2025-07-01\n1048\n\n\n9\n2025-08-01\n2549"
  },
  {
    "objectID": "notebooks/flat_ads.html#rents",
    "href": "notebooks/flat_ads.html#rents",
    "title": "WG Gesucht Stats",
    "section": "Rents",
    "text": "Rents\n\n\nCode\n# Define helper to filter outliers in rent\ndef filter_outliers(series, k=1.5):\n    q1 = series.quantile(0.25)\n    q3 = series.quantile(0.75)\n    iqr = q3 - q1\n    lower = q1 - k * iqr\n    upper = q3 + k * iqr\n    return series[(series &gt;= lower) & (series &lt;= upper)]\n\n# Apply filter just for rent histogram\nrent_filtered = filter_outliers(df[\"rent\"])\n\n# Recompute stats\ntime_min = pd.to_datetime(df[\"published\"]).min().date()\ntime_max = pd.to_datetime(df[\"published\"]).max().date()\nmin_rent = rent_filtered.min()\nmax_rent = rent_filtered.max()\navg_rent = rent_filtered.mean()\nmedian_rent = rent_filtered.median()\n\n# Plot histogram\nplt.hist(rent_filtered, bins=10, edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Rent (€)\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of Rents ({time_min} – {time_max}, outliers removed)\")\n\n# Add lines for stats\nplt.axvline(min_rent, color=\"blue\", linestyle=\"dashed\", linewidth=1, label=f\"Min: {min_rent:.0f}€\")\nplt.axvline(max_rent, color=\"red\", linestyle=\"dashed\", linewidth=1, label=f\"Max: {max_rent:.0f}€\")\nplt.axvline(avg_rent, color=\"green\", linestyle=\"dashed\", linewidth=1, label=f\"Avg: {avg_rent:.1f}€\")\nplt.axvline(median_rent, color=\"purple\", linestyle=\"dashed\", linewidth=1, label=f\"Median: {median_rent:.1f}€\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#sizes",
    "href": "notebooks/flat_ads.html#sizes",
    "title": "WG Gesucht Stats",
    "section": "Sizes",
    "text": "Sizes\n\n\nCode\nsize_filtered = filter_outliers(df[\"size\"])\n\n# Calculate stats for size\nmin_size = size_filtered.min()\nmax_size = size_filtered.max()\navg_size = size_filtered.mean()\nmedian_size = size_filtered.median()\n\n# Plot histogram for sizes\nplt.hist(size_filtered, bins=10, edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Size (m²)\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of Sizes ({time_min} – {time_max}, outliers removed)\")\n\n# Add lines for stats\nplt.axvline(min_size, color=\"blue\", linestyle=\"dashed\", linewidth=1, label=f\"Min: {min_size:.0f} m²\")\nplt.axvline(max_size, color=\"red\", linestyle=\"dashed\", linewidth=1, label=f\"Max: {max_size:.0f} m²\")\nplt.axvline(avg_size, color=\"green\", linestyle=\"dashed\", linewidth=1, label=f\"Avg: {avg_size:.1f} m²\")\nplt.axvline(median_size, color=\"purple\", linestyle=\"dashed\", linewidth=1, label=f\"Median: {median_size:.1f} m²\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#inhabitants",
    "href": "notebooks/flat_ads.html#inhabitants",
    "title": "WG Gesucht Stats",
    "section": "Inhabitants",
    "text": "Inhabitants\n\n\nCode\n# Minimum inhabitants to consider\nmin_inh_allowed = 1\n\ninh_filtered = df[\"total_inhabitants\"][df[\"total_inhabitants\"] &gt;= min_inh_allowed]\n\n# Calculate stats\nmin_inh = inh_filtered.min()\nmax_inh = inh_filtered.max()\navg_inh = inh_filtered.mean()\nmedian_inh = inh_filtered.median()\n\n# Plot histogram\nplt.hist(\n    inh_filtered,\n    bins=range(int(min_inh), int(max_inh) + 2),\n    edgecolor=\"black\",\n    alpha=0.7,\n    align=\"left\",\n    rwidth=0.8\n)\nplt.xlabel(\"Number of Inhabitants\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of Inhabitants per WG ({time_min} – {time_max}, min={min_inh_allowed})\")\n\n# Add lines for stats\nplt.axvline(min_inh, color=\"blue\", linestyle=\"dashed\", linewidth=1, label=f\"Min: {min_inh:.0f}\")\nplt.axvline(max_inh, color=\"red\", linestyle=\"dashed\", linewidth=1, label=f\"Max: {max_inh:.0f}\")\nplt.axvline(avg_inh, color=\"green\", linestyle=\"dashed\", linewidth=1, label=f\"Avg: {avg_inh:.1f}\")\nplt.axvline(median_inh, color=\"purple\", linestyle=\"dashed\", linewidth=1, label=f\"Median: {median_inh:.1f}\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute average composition per ad (normalize per WG)\navg_female = (df[\"female_inhabitants\"] / df[\"total_inhabitants\"]).mean()\navg_male = (df[\"male_inhabitants\"] / df[\"total_inhabitants\"]).mean()\navg_diverse = (df[\"diverse_inhabitants\"] / df[\"total_inhabitants\"]).mean()\n\n# Data for pie chart\navg_composition = [avg_female, avg_male, avg_diverse]\nlabels = [\"Female\", \"Male\", \"Diverse\"]\ncolors = [\"pink\", \"skyblue\", \"orange\"]\n\nplt.pie(avg_composition, labels=labels, autopct=\"%.1f%%\", colors=colors, startangle=90)\nplt.title(f\"Average Inhabitant Composition per WG ({time_min} – {time_max})\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Categorize ads\ndef wg_type(row):\n    f, m, d = row[\"female_inhabitants\"], row[\"male_inhabitants\"], row[\"diverse_inhabitants\"]\n    if f &gt; 0 and m == 0 and d == 0:\n        return \"All-female\"\n    elif m &gt; 0 and f == 0 and d == 0:\n        return \"All-male\"\n    else:\n        return \"Mixed\"\n\ndf[\"wg_type\"] = df.apply(wg_type, axis=1)\n\n# Count ads per category\nwg_counts = df[\"wg_type\"].value_counts()\n\n# Pie chart with numbers\nlabels = [f\"{cat} ({count})\" for cat, count in wg_counts.items()]\nplt.pie(wg_counts, labels=labels, autopct=\"%.1f%%\", startangle=90, colors=[\"pink\",\"skyblue\",\"lightgreen\"])\nplt.title(f\"WG Type Distribution ({time_min} – {time_max})\")\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#districts",
    "href": "notebooks/flat_ads.html#districts",
    "title": "WG Gesucht Stats",
    "section": "Districts",
    "text": "Districts\n\n\nCode\ndistrict_counts_df = df[\"district\"].value_counts().reset_index()\ndistrict_counts_df\n\n\n\n\n\n\n\n\n\ndistrict\ncount\n\n\n\n\n0\nNeukölln\n342\n\n\n1\nPrenzlauer Berg\n314\n\n\n2\nFriedrichshain\n284\n\n\n3\nKreuzberg\n243\n\n\n4\nMitte\n222\n\n\n...\n...\n...\n\n\n202\nAlt-Tegel\n1\n\n\n203\nKaulsdorf\n1\n\n\n204\nDanzigerstr / Prenzallee\n1\n\n\n205\nRosenthal\n1\n\n\n206\nPankow/Prenzlauer Berg\n1\n\n\n\n\n207 rows × 2 columns\n\n\n\n\n\nCode\n# Take top 10 districts by count\ntop10_districts = district_counts_df.head(10).set_index(\"district\")[\"count\"]\n# Pie chart with counts included in labels\ntop10_labels = [f\"{district} ({count})\" for district, count in top10_districts.items()]\n\nplt.pie(\n    top10_districts,\n    labels=top10_labels,\n    autopct=\"%.1f%%\",\n    startangle=90\n)\nplt.title(f\"Top 10 Districts by Number of Ads ({time_min} – {time_max})\")\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#spatialdistrict-analysis",
    "href": "notebooks/flat_ads.html#spatialdistrict-analysis",
    "title": "WG Gesucht Stats",
    "section": "Spatial/District Analysis",
    "text": "Spatial/District Analysis\n\n\nCode\ngdf = gpd.read_file(\"data/berlin_ortsteile.geojson\")\nprint(gdf.columns)\nprint(gdf.head())\n\n\nIndex(['gml_id', 'spatial_name', 'spatial_alias', 'spatial_type', 'OTEIL',\n       'BEZIRK', 'FLAECHE_HA', 'geometry'],\n      dtype='object')\n             gml_id spatial_name spatial_alias spatial_type         OTEIL  \\\n0  re_ortsteil.0101         0101         Mitte      Polygon         Mitte   \n1  re_ortsteil.0102         0102        Moabit      Polygon        Moabit   \n2  re_ortsteil.0103         0103  Hansaviertel      Polygon  Hansaviertel   \n3  re_ortsteil.0104         0104    Tiergarten      Polygon    Tiergarten   \n4  re_ortsteil.0105         0105       Wedding      Polygon       Wedding   \n\n  BEZIRK  FLAECHE_HA                                           geometry  \n0  Mitte   1063.8748  POLYGON ((13.41649 52.52696, 13.41635 52.52702...  \n1  Mitte    768.7909  POLYGON ((13.33884 52.51974, 13.33884 52.51974...  \n2  Mitte     52.5337  POLYGON ((13.34322 52.51557, 13.34323 52.51557...  \n3  Mitte    516.0672  POLYGON ((13.36879 52.49878, 13.36891 52.49877...  \n4  Mitte    919.9112  POLYGON ((13.34656 52.53879, 13.34664 52.53878...  \n\n\n\n\nCode\nunique_ortsteile = gdf[\"OTEIL\"].unique()\nunique_ortsteile.sort()\n\ndf_unique_ortsteile = pd.DataFrame(unique_ortsteile, columns=[\"Ortsteil\"])\ndf_unique_ortsteile\n\n\n\n\n\n\n\n\n\nOrtsteil\n\n\n\n\n0\nAdlershof\n\n\n1\nAlt-Hohenschönhausen\n\n\n2\nAlt-Treptow\n\n\n3\nAltglienicke\n\n\n4\nBaumschulenweg\n\n\n...\n...\n\n\n91\nWilhelmsruh\n\n\n92\nWilhelmstadt\n\n\n93\nWilmersdorf\n\n\n94\nWittenau\n\n\n95\nZehlendorf\n\n\n\n\n96 rows × 1 columns\n\n\n\n\n\nCode\n# Just the non-specific \"Berlin\" Ortsteil is not included ofc\ntop_districts = set(district_counts_df.head(20)[\"district\"].tolist())\nnot_in_ortsteile = top_districts - set(unique_ortsteile.tolist())\nnot_in_ortsteile\n\n\n{'Berlin',\n 'Charlottenburg-Wilmersdorf',\n 'Friedrichshain-Kreuzberg',\n 'Tempelhof-Schöneberg'}\n\n\n\n\nCode\ndistricts_to_visualize = top_districts - not_in_ortsteile\ndistricts_to_visualize\n\n\n{'Charlottenburg',\n 'Friedrichshain',\n 'Kreuzberg',\n 'Lichtenberg',\n 'Mitte',\n 'Moabit',\n 'Neukölln',\n 'Pankow',\n 'Prenzlauer Berg',\n 'Reinickendorf',\n 'Schöneberg',\n 'Spandau',\n 'Steglitz',\n 'Tempelhof',\n 'Wedding',\n 'Wilmersdorf'}\n\n\n\n\nCode\n# keep only top-x to show\ntop10 = district_counts_df[district_counts_df[\"district\"].isin(districts_to_visualize)]\n\n# merge counts onto geometries\ngdf_top = gdf.merge(top10, left_on=\"OTEIL\", right_on=\"district\", how=\"inner\")\n\n# nice, consistent light→strong coloring\nvmin, vmax = gdf_top[\"count\"].min(), gdf_top[\"count\"].max()\n\n# background\nax = gdf.plot(edgecolor=\"black\", linewidth=0.4, alpha=0.05, figsize=(8, 8))\n\n# choropleth (no legend), consistent cmap range\ngdf_top.plot(\n    ax=ax,\n    column=\"count\",\n    cmap=\"Oranges\",\n    vmin=vmin,\n    vmax=vmax,\n    edgecolor=\"black\",\n    linewidth=0.8,\n    alpha=0.9,\n)\n\n# labels (count + name) with white stroke for readability\nfor _, r in gdf_top.iterrows():\n    x, y = r.geometry.representative_point().coords[0]\n    ax.text(\n        x, y, f\"{int(r['count'])}\\n{r['OTEIL']}\",\n        ha=\"center\", va=\"center\", fontsize=9, linespacing=0.9,\n        path_effects=[pe.withStroke(linewidth=2.5, foreground=\"white\")]\n    )\n\n# zoom to selected\nxmin, ymin, xmax, ymax = gdf_top.total_bounds\nax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n\nax.set_title(f\"Top Districts by Ad Count ({time_min} – {time_max})\")\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# avg size per district\navg_size_df = (\n    df.groupby(\"district\", as_index=False)[\"size\"]\n      .median()\n      .rename(columns={\"size\": \"avg_sqm\"})\n)\n\n# keep only top-10 to show\ntop10_avg = avg_size_df[avg_size_df[\"district\"].isin(districts_to_visualize)]\n\n# merge onto geometries\ngdf_avg = gdf.merge(top10_avg, left_on=\"OTEIL\", right_on=\"district\", how=\"inner\")\n\n# consistent light→strong color scale\nvmin, vmax = gdf_avg[\"avg_sqm\"].min(), gdf_avg[\"avg_sqm\"].max()\n\n# background\nax = gdf.plot(edgecolor=\"black\", linewidth=0.4, alpha=0.05, figsize=(8, 8))\n\n# choropleth (no legend)\ngdf_avg.plot(\n    ax=ax,\n    column=\"avg_sqm\",\n    cmap=\"Blues\",\n    vmin=vmin,\n    vmax=vmax,\n    edgecolor=\"black\",\n    linewidth=0.8,\n    alpha=0.9,\n)\n\n# labels: avg m² + name\nfor _, r in gdf_avg.iterrows():\n    x, y = r.geometry.representative_point().coords[0]\n    ax.text(\n        x, y, f\"{r['avg_sqm']:.1f} m²\\n{r['OTEIL']}\",\n        ha=\"center\", va=\"center\", fontsize=9, linespacing=0.9,\n        path_effects=[pe.withStroke(linewidth=2.5, foreground=\"white\")]\n    )\n\n# zoom to selected\nxmin, ymin, xmax, ymax = gdf_avg.total_bounds\nax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n\nax.set_title(\"Top Districts by Median Room Size (m²)\")\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\navg_rent_df = (\n    df.groupby(\"district\", as_index=False)[\"rent\"]\n      .median()\n      .rename(columns={\"rent\": \"avg_rent\"})\n)\n\ntop10_avg_rent = avg_rent_df[avg_rent_df[\"district\"].isin(districts_to_visualize)]\ngdf_avg_rent = gdf.merge(top10_avg_rent, left_on=\"OTEIL\", right_on=\"district\", how=\"inner\")\n\nvmin, vmax = gdf_avg_rent[\"avg_rent\"].min(), gdf_avg_rent[\"avg_rent\"].max()\n\nax = gdf.plot(edgecolor=\"black\", linewidth=0.4, alpha=0.05, figsize=(8, 8))\ngdf_avg_rent.plot(\n    ax=ax, column=\"avg_rent\", cmap=\"Reds\", vmin=vmin, vmax=vmax,\n    edgecolor=\"black\", linewidth=0.8, alpha=0.9\n)\n\nfor _, r in gdf_avg_rent.iterrows():\n    x, y = r.geometry.representative_point().coords[0]\n    ax.text(\n        x, y, f\"{r['avg_rent']:.0f}€\\n{r['OTEIL']}\",\n        ha=\"center\", va=\"center\", fontsize=9, linespacing=0.9,\n        path_effects=[pe.withStroke(linewidth=2.5, foreground=\"white\")]\n    )\n\nxmin, ymin, xmax, ymax = gdf_avg_rent.total_bounds\nax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\nax.set_title(\"Top Districts by Median Rent (€)\")\nax.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Welcome to WG Gesucht Stats!",
    "section": "",
    "text": "Use the navigation bar to open the data analyses."
  }
]