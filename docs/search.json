[
  {
    "objectID": "notebooks/proxy_check.html",
    "href": "notebooks/proxy_check.html",
    "title": "Check proxies",
    "section": "",
    "text": "Code\nimport re, os, requests\nimport concurrent.futures as cf\nfrom tqdm import tqdm\n\nFREE_PROXY_LIST_URL = \"https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/socks4.txt\"\nTARGET_URL = \"https://api.ipify.org\"  # tiny endpoint, returns your IP\nCONNECT_TIMEOUT, READ_TIMEOUT = 3, 5\nMAX_WORKERS = 64  # tune based on ulimit / OS\n\n\n\n\nCode\ndef load_proxies(url=FREE_PROXY_LIST_URL):\n    txt = requests.get(url, timeout=(5,10)).text\n    return list({p for p in re.findall(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}:\\d{2,5}\\b\", txt)})\n\ndef load_proxies_from_file(path: str) -&gt; list[str]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        lines = f.read().splitlines()\n    # keep only host:port patterns\n    return [line.strip() for line in lines if line.strip() and \":\" in line]\n\n\nproxies = load_proxies_from_file(\"out/working_proxies_all.txt\")\nprint(f\"Loaded {len(proxies)} proxies\")\n\n\nLoaded 269 proxies\n\n\n\n\nCode\ndef is_alive(proxy):\n    px = {\"http\": f\"{proxy}\", \"https\": f\"{proxy}\"}\n    try:\n        r = requests.get(TARGET_URL, proxies=px, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT), allow_redirects=False)\n        return proxy if (r.status_code == 200 and r.text.strip()) else None\n    except requests.RequestException:\n        return None\n\n\n\n\nCode\nalive = []\nwith cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n    for res in tqdm(ex.map(is_alive, proxies, chunksize=50), total=len(proxies)):\n        if res: alive.append(res)\n\nprint(f\"Working proxies: {len(alive)}\")\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 269/269 [00:12&lt;00:00, 22.28it/s]\n\n\nWorking proxies: 100\n\n\n\n\n\n\n\nCode\nos.makedirs(\"out\", exist_ok=True)\nwith open(\"out/working_proxies.txt\", \"w\") as f:\n    f.write(\"\\n\".join(alive))\n\nprint(\"Saved working proxies to out/working_proxies.txt\")\n\n\nSaved working proxies to out/working_proxies.txt"
  },
  {
    "objectID": "notebooks/flat_ad_details.html",
    "href": "notebooks/flat_ad_details.html",
    "title": "Diving in deeper",
    "section": "",
    "text": "Code\nimport logging\nimport sys\nfrom dataclasses import asdict\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom tqdm.contrib.concurrent import thread_map\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport re, spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\nfrom sentence_transformers import SentenceTransformer\nimport umap\nimport hdbscan\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\nfrom tqdm.auto import tqdm\nimport torch\nfrom wordcloud import WordCloud, STOPWORDS\nfrom IPython.display import display\n\nfrom wggesuchtstats.scraper import get_flat_details\n\n# logging.getLogger(\"backoff\").setLevel(logging.WARNING) \nlogging.getLogger(\"urllib3.connectionpool\").setLevel(logging.WARNING)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\n\ntqdm.pandas()\nload_dotenv()\n\n\nFalse"
  },
  {
    "objectID": "notebooks/flat_ad_details.html#get-the-data",
    "href": "notebooks/flat_ad_details.html#get-the-data",
    "title": "Diving in deeper",
    "section": "Get the data",
    "text": "Get the data\n\n\nCode\n# --- Load & fix URLs ---\ndf = pd.read_csv(\"../data/flat_ads.csv\").reset_index(drop=True)\ndf[\"url\"] = df[\"url\"].apply(\n    lambda u: u if str(u).startswith(\"http\") else f\"https://www.wg-gesucht.de/{str(u).lstrip('/')}\"\n)\n\n\ndf[\"details\"] = thread_map(\n    get_flat_details,\n    df[\"url\"].tolist(),\n    max_workers=64,\n    desc=\"Fetching ads\",\n    chunksize=1,\n)\n\n# --- Expand dataclass fields ---\ndetails_df = df[\"details\"].apply(lambda d: asdict(d) if d is not None else {})\ndf = df.join(pd.json_normalize(details_df)).drop(columns=[\"details\"])\n\n# --- Save ---\ndf.to_csv(\"../data/flat_ads_with_details_new.csv\", index=False, encoding=\"utf-8\")\nlen(df)\n\n\nFetching ads: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1766/1766 [07:59&lt;00:00,  3.68it/s]\n\n\n\n\nCode\ndf = pd.read_csv(\"../data/flat_ads_with_details.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nurl\npublished\nrent\nsize\ndistrict\nfemale_inhabitants\nmale_inhabitants\ndiverse_inhabitants\ntotal_inhabitants\nheadline\ndescription\nstreet\nzip_code\navailable_from\navailable_until\nage_min\nage_max\n\n\n\n\n0\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-16T00:00:00\n550\n16\nPrenzlauer Berg\n1\n0\n0\n2\nNUR Berufst√§tige/Praktikanten: 2-er WG in Pre...\n16qm,3m Zimmerh√∂he, S√ºdosten, 4.OG, sehr hell ...\nDanziger Str\n10407.0\n2025-09-01\n2025-12-01\n49.0\n49.0\n\n\n1\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-16T00:00:00\n999\n20\nPrenzlauer Berg\n1\n1\n0\n4\nURBANELITE.COM // Keine Kaution! (keine Absich...\n- &gt; WWW. URBANELITE.COM \\n\\n WIR ARE: \\n Wir s...\nSch√∂nhauser Allee\n10439.0\n2025-08-16\nNaN\nNaN\nNaN\n\n\n2\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-16T00:00:00\n650\n20\nPrenzlauer Berg\n0\n1\n0\n2\nBright Room for Sublet in Prenzlauer Berg (6 M...\nIch biete meine ca. 20 m2 Zimmer in einer reno...\nDunckerstra√üe\n10439.0\n2025-10-01\n2026-03-31\n35.0\n40.0\n\n\n3\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-08-15T00:00:00\n750\n16\nFriedrichshain\n0\n1\n0\n2\nGem√ºtliches Sonnenzimmer in Fshain\nMieten Mein sch√∂n sonniges Zimmer ‚Äì 2 Monate (...\nBosestra√üe 6A\n10245.0\n2025-09-01\n2026-04-01\n30.0\n36.0\n\n\n4\nhttps://www.wg-gesucht.de/wg-zimmer-in-Berlin-...\n2025-07-14T00:00:00\n900\n20\nKreuzberg\n1\n0\n0\n2\nWG room 20m2 Mehringdamm\nZimmer verf√ºgbar ab September. \\n Letztendlich...\nGro√übeerenstra√üe 28\n10965.0\n2025-08-15\nNaN\n28.0\n28.0\n\n\n\n\n\n\n\n\n\nCode\ndf = df[df[\"headline\"].notna()] \nlen(df)\n\n\n3747"
  },
  {
    "objectID": "notebooks/flat_ad_details.html#age",
    "href": "notebooks/flat_ad_details.html#age",
    "title": "Diving in deeper",
    "section": "Age",
    "text": "Age\n\n\nCode\ndef get_stats(series):\n    return {\n        \"min\": series.min(),\n        \"max\": series.max(),\n        \"mean\": series.mean(),\n        \"median\": series.median()\n    }\n\nage_min_stats = get_stats(df[\"age_min\"].dropna())\nage_max_stats = get_stats(df[\"age_max\"].dropna())\n\n# Histogram with legend showing stats\nplt.figure(figsize=(8,5))\nsns.histplot(df[\"age_min\"], color=\"blue\", kde=True, alpha=0.5, label=\"age_min\")\nsns.histplot(df[\"age_max\"], color=\"red\", kde=True, alpha=0.5, label=\"age_max\")\n\n# Add stats to legend\nlegend_text = [\n    f\"age_min: min:{age_min_stats['min']}, max:{age_min_stats['max']}, \"\n    f\"mean:{age_min_stats['mean']:.1f}, median:{age_min_stats['median']}\",\n    f\"age_max: min:{age_max_stats['min']}, max:{age_max_stats['max']}, \"\n    f\"mean:{age_max_stats['mean']:.1f}, median:{age_max_stats['median']}\"\n]\n\nplt.legend(legend_text)\nplt.title(\"Min/max age of inhabitants\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_cleaned = df.dropna(subset=[\"age_min\",\"age_max\"])\nX = df_cleaned[[\"age_min\",\"age_max\"]]\n\n# --- Fit KMeans ---\nbest_k = 3  # &lt;- set this after elbow method\nkmeans = KMeans(n_clusters=best_k, random_state=42, n_init=\"auto\")\ndf_cleaned[\"cluster\"] = kmeans.fit_predict(X)\n\ncluster_names = {\n    0: \"Students/Young Professionals\",\n    1: \"Cross-generational\",\n    2: \"Working-age\"\n}\n\n# Map numeric cluster labels to names\ndf_cleaned[\"cluster_name\"] = df_cleaned[\"cluster\"].map(cluster_names)\n\n# --- Cluster sizes ---\ncluster_counts = df_cleaned[\"cluster\"].value_counts(normalize=True).sort_index() * 100\n\n# --- Scatterplot ---\nplt.figure(figsize=(7,6))\nsns.scatterplot(data=df_cleaned, x=\"age_min\", y=\"age_max\", hue=\"cluster_name\", palette=\"Set2\", alpha=0.6)\n\n# cluster centers\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:,0], centers[:,1], c=\"black\", s=150, marker=\"X\", label=\"Centers\")\n\n# annotate percentages\nfor idx, (x, y) in enumerate(centers):\n    pct = cluster_counts[idx]\n    plt.text(x, y+1, f\"{pct:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\", color=\"black\")\n\nplt.plot([0,60],[0,60], color=\"gray\", linestyle=\"--\")\nplt.xlabel(\"age_min\")\nplt.ylabel(\"age_max\")\nplt.title(f\"KMeans Clusters of WG Ads (k={best_k})\")\nplt.legend()\nplt.show()\n\n\nC:\\Users\\Yannik\\AppData\\Local\\Temp\\ipykernel_12704\\2865640186.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cleaned[\"cluster\"] = kmeans.fit_predict(X)\nC:\\Users\\Yannik\\AppData\\Local\\Temp\\ipykernel_12704\\2865640186.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_cleaned[\"cluster_name\"] = df_cleaned[\"cluster\"].map(cluster_names)"
  },
  {
    "objectID": "notebooks/flat_ad_details.html#nlp",
    "href": "notebooks/flat_ad_details.html#nlp",
    "title": "Diving in deeper",
    "section": "NLP",
    "text": "NLP\n\n\nCode\ndf[\"text\"] = (df[\"headline\"].fillna(\"\") + \" \" + df[\"description\"].fillna(\"\")).str.strip()\ndf_nlp = df[df[\"text\"].str.len() &gt; 0].copy()\n\n\n\n\nCode\n\n\nnlp = spacy.load(\"de_core_news_sm\", disable=[\"parser\"])\nSTOP_EXTRA = set(line.strip() for line in open(\"../data/stop_words.txt\", encoding=\"utf-8\"))\n\ndef clean_txt(s: str) -&gt; str:\n    s = s.lower()\n    s = re.sub(r\"\\s+\", \" \", s)\n    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n    s = re.sub(r\"[\\d_]+\", \" \", s)\n    return s.strip()\n\ndf_nlp[\"text_clean\"] = df_nlp[\"text\"].progress_apply(clean_txt)\ntexts = df_nlp[\"text_clean\"].tolist()\nlemmas = []\nfor doc in tqdm(nlp.pipe(texts, batch_size=64), total=len(texts), desc=\"lemmatizing\"):\n    toks = [\n        t.lemma_.lower()\n        for t in doc\n        if t.is_alpha and not t.is_stop and len(t.lemma_) &gt;= 3 and t.lemma_.lower() not in STOP_EXTRA\n    ]\n    lemmas.append(\" \".join(toks))\n\ndf_nlp[\"text_lem\"] = lemmas\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3747/3747 [00:00&lt;00:00, 3882.27it/s]\nlemmatizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3747/3747 [04:03&lt;00:00, 15.38it/s]\n\n\n\n\nCode\ndef top_tfidf_terms(texts, n=20, ngram=(1,1), min_df=5, max_df=0.6):\n    v = TfidfVectorizer(ngram_range=ngram, min_df=min_df, max_df=max_df)\n    X = v.fit_transform(texts)\n    mean_scores = np.asarray(X.mean(axis=0)).ravel()\n    idx = mean_scores.argsort()[::-1][:n]\n    return pd.DataFrame({\"term\": np.array(v.get_feature_names_out())[idx],\n                         \"tfidf\": mean_scores[idx]})\n\ntop_uni = top_tfidf_terms(df_nlp[\"text_lem\"], n=25, ngram=(1,1))\ntop_bi  = top_tfidf_terms(df_nlp[\"text_lem\"], n=25, ngram=(2,2))\nprint(top_uni.head(10)); print(top_bi.head(10))\n\n\n       term     tfidf\n0     ruhig  0.028846\n1     sch√∂n  0.027227\n2     k√ºche  0.022431\n3    balkon  0.021267\n4    liegen  0.021082\n5    direkt  0.020641\n6    suchen  0.020607\n7    person  0.019227\n8  befinden  0.018281\n9  entfernt  0.017532\n                        term     tfidf\n0            prenzlauer berg  0.012642\n1            fully furnished  0.007785\n2                  k√ºche bad  0.007551\n3             voll m√∂blieren  0.007408\n4  √∂ffentlich verkehrsmittel  0.007113\n5          voll ausgestattet  0.007092\n6          bett schreibtisch  0.007062\n7         ausgestattet k√ºche  0.006857\n8           tempelhofer feld  0.006396\n9                hoch decken  0.006232\n\n\n\n\nCode\nstopword_list = STOPWORDS.union(STOP_EXTRA)\n\n# TF-IDF ‚Üí mean weight per term\nv = TfidfVectorizer(ngram_range=(1,1), min_df=5, max_df=0.6)\nX = v.fit_transform(df_nlp[\"text_lem\"])\nweights = np.asarray(X.mean(axis=0)).ravel()\nterms = v.get_feature_names_out()\nfreq = {t: float(w) for t, w in zip(terms, weights)}\n\nwc = WordCloud(width=1400, height=900, background_color=\"white\", stopwords=stopword_list)\nimg = wc.generate_from_frequencies(freq)\n\nplt.figure(figsize=(12,8))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"WG Ads ‚Äî TF-IDF Word Cloud (unigrams)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef lsa_topics(texts, n_topics=8, n_terms=10, min_df=5, max_df=0.6, ngram=(1,2)):\n    v = TfidfVectorizer(ngram_range=ngram, min_df=min_df, max_df=max_df)\n    X = v.fit_transform(texts)\n    svd = TruncatedSVD(n_components=n_topics, random_state=42)\n    W = svd.fit_transform(X)\n    H = svd.components_\n    terms = v.get_feature_names_out()\n    topics = []\n    for k in range(n_topics):\n        idx = np.argsort(H[k])[::-1][:n_terms]\n        topics.append({\"topic\": k, \"terms\": \", \".join(terms[i] for i in idx)})\n    doc_topics = W.argmax(axis=1)\n    return pd.DataFrame(topics), pd.Series(doc_topics, name=\"topic\")\n\ntopics_df, df_nlp[\"topic_lsa\"] = lsa_topics(df_nlp[\"text_lem\"], n_topics=6, n_terms=12)\nprint(topics_df)\n\n\n   topic                                              terms\n0      0  ruhig, sch√∂n, k√ºche, from, direkt, suchen, per...\n1      1  our, your, flat, from, can, community, have, t...\n2      2  community, our, hinaus, netzwerk, network, mor...\n3      3  mal, wichtig, leben, that, mensch, but, kochen...\n4      4  prenzlauer, berg, prenzlauer berg, allee, sch√∂...\n5      5  august, neuk√∂lln, september, kreuzberg, sch√∂n,...\n\n\n\n\nCode\nmodel = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\nemb = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)\n\num = umap.UMAP(n_neighbors=15, min_dist=0.05, n_components=2, random_state=42)\nxy = um.fit_transform(emb)\n\ncl = hdbscan.HDBSCAN(min_cluster_size=30, min_samples=10, metric=\"euclidean\").fit(emb)\ndf_nlp[\"cluster_hdb\"] = cl.labels_  # -1 = noise\n\n# Inspect clusters\nprint(df_nlp[\"cluster_hdb\"].value_counts().head(10))\n\n\n2025-08-19 14:25:55,237 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n2025-08-19 14:25:55,239 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n\n\nBatches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:04&lt;00:00,  2.07s/it]\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n  warn(\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n  warnings.warn(\n\n\ncluster_hdb\n 0    2632\n-1    1075\n 1      40\nName: count, dtype: int64\n\n\n\n\nCode\ndef cluster_keywords(texts, labels, topn=12):\n    out = []\n    for lab in sorted(set(labels)):\n        if lab == -1: continue\n        subset = [t for t,l in zip(texts, labels) if l == lab]\n        tt = top_tfidf_terms(subset, n=topn, ngram=(1,2))\n        out.append((lab, \", \".join(tt.term.tolist())))\n    return pd.DataFrame(out, columns=[\"cluster\",\"keywords\"])\n    \nprint(cluster_keywords(df_nlp[\"text_lem\"].tolist(), df_nlp[\"cluster_hdb\"].tolist()))\n\n\n   cluster                                           keywords\n0        0  ruhig, sch√∂n, k√ºche, liegen, balkon, direkt, s...\n1        1  friedrichshain, kreuzberg, friedrichshain idea...\n\n\n\n\nCode\nMODEL = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\ntorch.set_num_threads(3)\ndevice = 0 if torch.cuda.is_available() else -1\n\ntok = AutoTokenizer.from_pretrained(MODEL)\nmdl = AutoModelForSequenceClassification.from_pretrained(MODEL)\npipe = TextClassificationPipeline(model=mdl, tokenizer=tok, device=device, return_all_scores=True)\n\ndef fast_sentiment(texts, batch_size=64, max_len=256, desc=\"Sentiment\"):\n    labels, scores = [], []\n    n_batches = (len(texts) + batch_size - 1) // batch_size\n    for i in tqdm(range(n_batches), desc=desc):\n        batch = texts[i*batch_size:(i+1)*batch_size]\n        out = pipe(batch, truncation=True, max_length=max_len)\n        for row in out:\n            best = max(row, key=lambda x: x[\"score\"])\n            labels.append(best[\"label\"].lower())\n            scores.append(float(best[\"score\"]))\n    return labels, scores\n\ndf_nlp[\"sentiment\"], df_nlp[\"sentiment_score\"] = fast_sentiment(df_nlp[\"text\"].tolist())\n\n\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n\n\n2025-08-19 16:08:27,175 - huggingface_hub.file_download - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n\n\nDevice set to use cpu\nc:\\Users\\Yannik\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\wggesuchtstats-9Efq763c-py3.12\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nSentiment: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [21:20&lt;00:00, 21.70s/it]\n\n\n\n\nCode\ndf_nlp[\"sentiment\"].value_counts(normalize=True).mul(100).round(1)\n\n\nsentiment\npositive    82.7\nnegative    16.2\nneutral      1.1\nName: proportion, dtype: float64\n\n\n\n\nCode\npd.set_option(\"display.max_colwidth\", None)  # no truncation\n\n# get most positive (highest score where label=positive)\npos = df_nlp[df_nlp[\"sentiment\"]==\"positive\"].sort_values(\"sentiment_score\", ascending=False).head(5)\n\n# get most negative (highest score where label=negative)\nneg = df_nlp[df_nlp[\"sentiment\"]==\"negative\"].sort_values(\"sentiment_score\", ascending=False).head(5)\n\n# select just relevant fields\ncompare = pd.DataFrame({\n    \"Most Positive\": pos[\"text\"].values,\n    \"Most Negative\": neg[\"text\"].values\n})\n\ndisplay(compare)\n\n\n\n\n\n\n\n\n\nMost Positive\nMost Negative\n\n\n\n\n0\nsch√∂nes WG- Zimmer in Berlin Prenzlauerberg, befristet sch√∂nes, helles Zimmer in Berlin Prenzlauer Berg mit Balkon\nWilmersdorf ist ruhig und angesagt! Das Zimmer ist m√∂bliert. Die √∂ffentlichen Verkehrsmittel sind fussl√§ufig in 3 Minuten erreichbar. Die Superm√§rkte in 5 Minuten.\n\n\n1\nRoom in Prenzlauer Berg Very nice and bright room, king size bed, plants, table and balcony\\nVery well located in the lively neighborhood of Prenzlauer Berg\\nKitchen\nPrivatzimmer 24 qm in Tempelhof nahe Ringbahn an eine Frau im Notfall auch fr√ºher m√∂glich aber auch etwas l√§nger Zeit punkt ist auch flexibel Das Zimmer ist gro√ü, hell und sehr ruhig es gibt 2 Zimmer und die Wohnung ist im 2. OG\\nIn dem sehr Gr√ºnen Bezirk ist es auch an den Wochenenden sehr leicht m√∂glich einen Einkauf zu erledigen, auch ist die N√§he zum Ring Bahnhof eine Garantie leicht √ºberall in Berlin hin zu kommen\\nIch bin ein Mann und 60 Jahre jung, durch eine Krankheit ( die nicht ansteckend ist ) , bin ich meist zuhause, versuche aber auch dennoch etwas zu finden das mich zeitweise besch√§ftigt. Ich suche wegen der doch sehr vielen schlechten Erfahrungen mit M√§nnern, auch wenn es anders besprochen wurde lieber eine Weibliche Mitbewohnerin.\\nKontakt ist jederzeit m√∂glich √ºber die Nachrichtenfunktion hier. Da es ja immer erst einmal ein Versuch des Zusammen Lebens ist kann auch etwas langfristiges daraus entstehen wenn man sich versteht\n\n\n2\nsch√∂nes helles Zimmer sch√∂nes helles Zimmer in Berlin Reinickendorf mit guter Infrastruktur.Die Wohnung liegt im Norden von Betlin.U Bahn Rathaus Reikickendorf ist 10 Minuten Fussl√§ufig entfernt.\nUntermiete 12.09.-27.10.25 in Berlin Wei√üensee 20qm mit 2 fenster und gro√üem bett. \\n Schriebtisch und sessel und viele pflanzen sind auch da - ich raume naturlich alles vorher aus und mach das fur dich nochmal schick - gleiches gilt f√ºr bad und k√ºche\\nRuhes und grunes weisensee \\n M2 - S prenzlauer Allee (5min) \\n M2 - Alexanderplatz (20min) \\n\\n - kannst aber auch ein rad von mir fur die zeit haben\\nMein mitbewohner ist ruhig und entspannt und korrekt. Er hat auch ein Veto recht, da er die zeit da ist.\\nZeit und kohle ist verhandlungssache\n\n\n3\nsch√∂n wg ab 15.august Hey! \\n Ich verlasse meine sch√∂ne kleine Wohnung, um nach Frankreich zu ziehen und zu studieren. \\n üîπDas Zimmer ist ca. 20m2, mit einem Verbindungsbalkon zum anderen Zimmer, Isa's, die immer sehr positiv und gro√üartig ist, um ein Glas Wein und einen guten Chat mit zu teilen. \\n Es gibt auch Mira, die gerne Musik h√∂ren und machen und immer ein gro√ües Lachen ist. \\n üî∫Das Zimmer ist ab 15. August verf√ºgbar! üî∫ \\n Die Wohnung befindet sich direkt im Zentrum von Berlin, in der N√§he aller Annehmlichkeiten! \\n üîπNur 8 Gehminuten vom Alexanderplatz und 5 Minuten vom U-Bahnhof jannowitzbr√ºcke entfernt. \\n üîπSeveral Superm√§rkte direkt auf der Stra√üe. \\n üî∏Senden Sie mir eine Einf√ºhrung zu sich selbst, und wir arrangieren einen Besuch zusammen! \\n\\n Freuen Sie sich auf eine Anh√∂rung von Ihnen (:\\n \\n\\n Heyy there üåû! \\n I'm leaving my lovely little flat-share to move back to France and study. \\n üîπThe room is about 20m2, with a connecting balcony to the other room, Isa's, who's always very positive and great to share a glass of wine and a good chat with. \\n There's also mira, who loves listening and making music, and is always a great laugh. \\n üî∫The room is available from August 15! üî∫ \\n The apartment is located right in the center of Berlin, close to all amenities! \\n üîπJust an 8-minute walk from Alexanderplatz and 5 minutes from the jannowitzbr√ºcke subway station. \\n üîπSeveral supermarkets right down the street. \\n üî∏Send me an introduction to yourself, and we'll arrange a visit together! \\n\\n Looking forward to hearing from you (:\\n \\n\\nText automatisch √ºbersetzt -\\nOriginal anzeigen\\n√úbersetzung anzeigen\nWohnen n√§he Volkspark Humboldthain / All-inclusive Zimmer: \\r\\n - Boxspringbett \\r\\n - gro√üer Kleiderschrank \\r\\n - Smart-TV \\r\\n - Schreibtisch \\r\\n - Balkon \\n\\r\\n K√ºche: \\r\\n - Einbauk√ºche mit Herd und Ofen \\r\\n - Geschirrsp√ºler \\r\\n - Waschmaschine \\r\\n - gro√üer K√ºhlschrank mit Gefrierkombination \\r\\n - Toaster, Kaffeemaschine etc. \\n\\r\\n Badezimmer: \\r\\n - Duschwanne \\r\\n - WC \\r\\n - W√§schespinne \\r\\n - B√ºgelbrett & B√ºgeleisen \\n\\n\\n\\r\\n Room: \\r\\n - Box spring bed \\r\\n - large closet \\r\\n - smart TV \\r\\n - desk \\r\\n - balcony \\n\\r\\n Kitchen: \\r\\n - Fitted kitchen with stove and oven \\r\\n - dishwasher \\r\\n - washing machine \\r\\n - large fridge with freezer combination \\r\\n - Toaster, coffee machine etc. \\n\\r\\n Bathroom: \\r\\n - Shower tray \\r\\n - WC \\r\\n - rotary dryer \\r\\n - Ironing board & iron\\nDas Objekt liegt aufstrebenden Ortsteil Gesundbrunnen, profitieren vom Ortsteil Mitte. Die Lage ist durch den Flair eines lebendigen und einfachen Anwohner gepr√§gt, das eine angenehme Wohnatmosph√§re bietet. \\n Das ist nicht gut. Sie von einer guten Nahverkehrsanbindung: Die U-Bahnlinie 9 ist fu√ül√§ufig innerhalb von ca. 8 erreichbar, was eine schnelle Verbindung ins Stadtzentrum und zu weiteren Teilen erm√∂glicht. Einrichtungen sich mehrere Bushaltestellen in der N√§he, mit den Linien M27, 147 und 255 genie√üen Sie bequem in allen Teilen der Hauptstadt. \\n In der n√§heren Umgebung finden Sie zahlreiche Einkaufsm√∂glichkeiten, darunter Supermarktketten mit guten Parkm√∂glichkeiten, die den t√§glichen Bed√ºrfnissen anpassen. vielf√§ltige Restaurants, Caf√©s und Dienstleistungsangebote in der N√§he vorhanden. \\n F√ºr Erholung und Freizeit bieten sich die nahegelegenen Fl√§chen Gr√ºn und Parks an: Der Volkspark Humboldthain mit seinem gro√üen Parkgel√§nde, den Wasserfl√§chen und dem bekannten Flakturm ist nur einige Minuten entfernt und l√§dt zu Spazierg√§nge, Jogging oder Picknicks ein. Auch der Sch√∂nhauser Park sowie der Gleisdreieckpark sind gut erreichbar und bieten vielf√§ltige M√∂glichkeiten f√ºr Freizeitaktivit√§ten im Gr√ºnen. \\n Die N√§he zu diesen Parks macht die Lage besonders lebenswert und bietet eine sch√∂ne Balance zwischen urbanem Leben und Natur. \\n\\n\\n\\n Das Apartment befindet sich im ankommenden Stadtteil Gesundbrunnen, direkt neben dem Stadtteil Mitte. Die Lage zeichnet sich durch das Flair einer lebhaften und dennoch ruhigen Wohngegend aus, die eine angenehme Wohnatmosph√§re bietet. \\n Gleichzeitig profitieren Sie von ausgezeichneten lokalen Verkehrsverbindungen: Die U-Bahnlinie 9 ist zu Fu√ü von ca. 8 Minuten, eine schnelle Verbindung zum Stadtzentrum und anderen Teilen der Stadt. Es gibt auch mehrere Bushaltestellen in der N√§he, mit den Linien M27, 147 und 255 bietet einfachen Zugang zu allen Teilen der Hauptstadt. \\n In der unmittelbaren Umgebung finden Sie zahlreiche Einkaufsm√∂glichkeiten, darunter Supermarktketten mit guten Parkpl√§tzen, die den t√§glichen Einkauf bequem machen. Es gibt auch verschiedene Restaurants, Caf√©s und Service-Outlets in der N√§he. \\n Die nahegelegenen Gr√ºnfl√§chen und Parks sind ideal f√ºr Erholung und Freizeit: Der Volkspark Humboldthain mit seiner gro√üen Parkanlage, Wasseranlagen und der ber√ºhmte Flakturm ist nur wenige Minuten entfernt und ist ideal f√ºr Spazierg√§nge, Joggen oder Picknicks. Der Sch√∂nhauser Park und der Gleisdreieckpark sind ebenfalls leicht zu erreichen und bieten eine breite Palette an Freizeitaktivit√§ten auf dem Land. \\n Die N√§he zu diesen Parks macht die Lage besonders lebendig und bietet eine sch√∂ne Balance zwischen urbanem Leben und Natur.\\n \\n\\n Das Objekt liegt im aufstrebenden Ortsteil Gesundbrunnen, unmittelbar benachbart zum Ortsteil Mitte. Die Lage ist durch den Flair eines lebendigen und dennoch ruhigen Anwohnergebietes gepr√§gt, das eine angenehme Wohnatmosph√§re bietet. \\r\\n Gleichzeitig profitieren Sie von einer ausgesprochen guten Nahverkehrsanbindung: Die U-Bahnlinie 9 ist fu√ül√§ufig innerhalb von ca. 8 Minuten erreichbar, was eine schnelle Verbindung ins Stadtzentrum und zu weiteren Stadtteilen erm√∂glicht. Zudem befinden sich mehrere Bushaltestellen in der N√§he, mit den Linien M27, 147 und 255 gelangen Sie bequem in alle Teile der Hauptstadt. \\r\\n In der n√§heren Umgebung finden Sie zahlreiche Einkaufsm√∂glichkeiten, darunter Supermarktketten mit guten Parkm√∂glichkeiten, die den t√§glichen Einkauf bequem machen. Zudem sind diverse Restaurants, Caf√©s und Dienstleistungsangebote in der N√§he vorhanden. \\r\\n F√ºr Erholung und Freizeit bieten sich die nahegelegenen Gr√ºnfl√§chen und Parks an: Der Volkspark Humboldthain mit seinem gro√üen Parkgel√§nde, den Wasserfl√§chen und dem bekannten Flakturm ist nur wenige Minuten entfernt und l√§dt zu Spazierg√§ngen, Jogging oder Picknicks ein. Auch der Sch√∂nhauser Park sowie der Gleisdreieckpark sind gut erreichbar und bieten vielf√§ltige M√∂glichkeiten f√ºr Freizeitaktivit√§ten im Gr√ºnen. \\r\\n Die N√§he zu diesen Parks macht die Lage besonders lebenswert und bietet eine sch√∂ne Balance zwischen urbanem Leben und Natur. \\n\\n\\n\\r\\n The apartment is located in the up-and-coming district of Gesundbrunnen, directly adjacent to the Mitte district. The location is characterized by the flair of a lively yet quiet residential area that offers a pleasant living atmosphere. \\r\\n At the same time, you benefit from excellent local transport connections: The subway line 9 is within walking distance of approx. 8 minutes, providing a quick connection to the city center and other parts of the city. There are also several bus stops nearby, with the M27, 147 and 255 lines providing easy access to all parts of the capital. \\r\\n In the immediate area, you will find numerous shopping facilities, including supermarket chains with good parking facilities, which make daily shopping convenient. There are also various restaurants, caf√©s and service outlets nearby. \\r\\n The nearby green spaces and parks are ideal for recreation and leisure: Volkspark Humboldthain with its large park area, water features and the famous flak tower is just a few minutes away and is ideal for walks, jogging or picnics. Sch√∂nhauser Park and Gleisdreieckpark are also within easy reach and offer a wide range of leisure activities in the countryside. \\r\\n The proximity to these parks makes the location particularly liveable and offers a nice balance between urban life and nature.\\n \\n\\nText automatisch √ºbersetzt -\\nOriginal anzeigen\\n√úbersetzung anzeigen\\nDas Zimmer befindet sich in einer ca. 63 m¬≤ gro√üen Wohnung im 2. OG eines modernen Neubaus in Wedding. \\r\\n Dein neues Zuhause ist vollst√§ndig ausgestattet und wird all-inclusive vermietet. \\n\\r\\n Der Mietpreis setzt sich aus den Kosten f√ºr die Wohnr√§ume und die M√∂blierung zusammen. Des Weiteren sind die Kosten f√ºr alle Neben-, Heiz- und Warmwasserkosten sowie die Stromkosten enthalten. Ein 200.000er Internetvertrag ist bereits abgeschlossen und inklusive. Wir √ºbernehmen au√üerdem die Beitragszahlungen an die GEZ. \\n\\r\\n Gerne stehen wir Ihnen, nach Vertragsabschluss f√ºr alle Probleme 10 Stunden Wochentags zur Verf√ºgung. \\n\\n\\n\\r\\n The room is located in an approx. 63 m¬≤ apartment on the 2nd floor of a modern new building in Wedding. \\r\\n Your new home is fully furnished and is rented all-inclusive. \\n\\r\\n The rent is made up of the costs for the living space and furnishings. It also includes all utilities, heating and hot water costs as well as electricity. A 200,000 internet contract has already been concluded and is included. We also pay the GEZ contributions. \\n\\r\\n We will be happy to help you with any problems 10 hours a week after the contract has been signed.\\nDie Mietdauer bel√§uft sich auf 6 bis 11 Monate. \\n\\r\\n Alle Angaben in diesem Expos√© (Objektbeschreibung, Abmessungen, Preisangaben etc.) beruhen auf Angaben des Eigent√ºmers und erfolgen ohne Gew√§hr. \\n\\r\\n Diese Wohnung wird nur f√ºr den vor√ºbergehenden Gebrauch zur Verf√ºgung gestellt. \\n\\n\\n\\r\\n The rental period is between 6 and 11 months. \\n\\r\\n All information in this expos√© (property description, dimensions, price details, etc.) is based on information provided by the owner and is given without guarantee. \\n\\r\\n This apartment is only made available for temporary use.\n\n\n4\nsch√∂nes zimmer NUR F√úR FRAUEN Linda habitaci√≥n con jard√≠n . \\r\\n Sch√∂nes Zimmer mit Gro√üe garten direkt am Frankfurter alle Boxagener Kiez, Kaffes Restaurants, Einkauf M√∂glichkeit ganz in der n√§he\nZimmer in Lankwitz (August/September) Da ich im August & September nicht in Berlin sein werde, vermiete ich mein Zimmer unter. Es ist m√∂bliert (Bett, Schreibtisch, Stuhl, Schrank, WLAN), Bad & K√ºche werden mit einer weiteren Frau geteilt.\n\n\n\n\n\n\n\n\n\nCode\ndef quick_ner_counts(texts, topn=20):\n    cats = []\n    for doc in nlp.pipe(texts, batch_size=64):\n        for e in doc.ents:\n            if e.label_ in {\"LOC\",\"PER\",\"ORG\",\"MISC\",\"DATE\",\"MONEY\"}:\n                cats.append((e.text.lower(), e.label_))\n    s = pd.DataFrame(cats, columns=[\"ent\",\"label\"]).value_counts().reset_index(name=\"count\")\n    return s.groupby(\"label\").head(topn)\n    \nner_top = quick_ner_counts(df_nlp[\"text\"].head(5000))  # sample for speed\nprint(ner_top.head(15))"
  },
  {
    "objectID": "notebooks/flat_ads.html",
    "href": "notebooks/flat_ads.html",
    "title": "WG Gesucht Stats",
    "section": "",
    "text": "last updated 17.08.2025\nSome statistics about the market in Berlin.\nCode\nimport logging\nimport sys\n\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as pe\nimport numpy as np\nimport geopandas as gpd\n\nfrom wggesuchtstats.scraper import find_shared_flats\nfrom wggesuchtstats.models import to_csv\n\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\n\nload_dotenv()\n\n\nTrue"
  },
  {
    "objectID": "notebooks/flat_ads.html#get-the-data",
    "href": "notebooks/flat_ads.html#get-the-data",
    "title": "WG Gesucht Stats",
    "section": "Get the data",
    "text": "Get the data\n\n\nCode\nflat_ads = find_shared_flats()\nto_csv(flat_ads, \"data/flat_ads.csv\")\nlen(flat_ads)"
  },
  {
    "objectID": "notebooks/flat_ads.html#clean-it",
    "href": "notebooks/flat_ads.html#clean-it",
    "title": "WG Gesucht Stats",
    "section": "Clean it",
    "text": "Clean it\n\n\nCode\n# Load CSV\ndf = pd.read_csv(\"../data/flat_ads.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nurl\npublished\nrent\nsize\ndistrict\nfemale_inhabitants\nmale_inhabitants\ndiverse_inhabitants\ntotal_inhabitants\n\n\n\n\n0\nwg-zimmer-in-Berlin-Prenzlauer-Berg.6103697.html\n2025-08-16T00:00:00\n550\n16\nPrenzlauer Berg\n1\n0\n0\n2\n\n\n1\nwg-zimmer-in-Berlin-Prenzlauer-Berg.9961003.html\n2025-08-16T00:00:00\n999\n20\nPrenzlauer Berg\n1\n1\n0\n4\n\n\n2\nwg-zimmer-in-Berlin-Prenzlauer-Berg.12248079.html\n2025-08-16T00:00:00\n650\n20\nPrenzlauer Berg\n0\n1\n0\n2\n\n\n3\nwg-zimmer-in-Berlin-Friedrichshain.12230915.html\n2025-08-15T00:00:00\n750\n16\nFriedrichshain\n0\n1\n0\n2\n\n\n4\nwg-zimmer-in-Berlin-Kreuzberg.12158342.html\n2025-07-14T00:00:00\n900\n20\nKreuzberg\n1\n0\n0\n2"
  },
  {
    "objectID": "notebooks/flat_ads.html#data-time-range",
    "href": "notebooks/flat_ads.html#data-time-range",
    "title": "WG Gesucht Stats",
    "section": "Data Time Range",
    "text": "Data Time Range\n\n\nCode\n# ensure datetime\ndf[\"published\"] = pd.to_datetime(df[\"published\"])\n\nmonthly_counts = (\n    df.groupby(df[\"published\"].dt.to_period(\"M\"))\n      .size()\n      .reset_index(name=\"count\")\n      .sort_values(\"published\")   # sort Periods\n)\n\n# convert Period to timestamp AFTER sorting\nmonthly_counts[\"published\"] = monthly_counts[\"published\"].dt.to_timestamp()\n\nmonthly_counts\n\n\n\n\n\n\n\n\n\npublished\ncount\n\n\n\n\n0\n2024-06-01\n1\n\n\n1\n2024-12-01\n1\n\n\n2\n2025-01-01\n1\n\n\n3\n2025-02-01\n3\n\n\n4\n2025-03-01\n7\n\n\n5\n2025-04-01\n9\n\n\n6\n2025-05-01\n31\n\n\n7\n2025-06-01\n116\n\n\n8\n2025-07-01\n1048\n\n\n9\n2025-08-01\n2549"
  },
  {
    "objectID": "notebooks/flat_ads.html#rents",
    "href": "notebooks/flat_ads.html#rents",
    "title": "WG Gesucht Stats",
    "section": "Rents",
    "text": "Rents\n\n\nCode\n# Define helper to filter outliers in rent\ndef filter_outliers(series, k=1.5):\n    q1 = series.quantile(0.25)\n    q3 = series.quantile(0.75)\n    iqr = q3 - q1\n    lower = q1 - k * iqr\n    upper = q3 + k * iqr\n    return series[(series &gt;= lower) & (series &lt;= upper)]\n\n# Apply filter just for rent histogram\nrent_filtered = filter_outliers(df[\"rent\"])\n\n# Recompute stats\ntime_min = pd.to_datetime(df[\"published\"]).min().date()\ntime_max = pd.to_datetime(df[\"published\"]).max().date()\nmin_rent = rent_filtered.min()\nmax_rent = rent_filtered.max()\navg_rent = rent_filtered.mean()\nmedian_rent = rent_filtered.median()\n\n# Plot histogram\nplt.hist(rent_filtered, bins=10, edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Rent (‚Ç¨)\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of Rents ({time_min} ‚Äì {time_max}, outliers removed)\")\n\n# Add lines for stats\nplt.axvline(min_rent, color=\"blue\", linestyle=\"dashed\", linewidth=1, label=f\"Min: {min_rent:.0f}‚Ç¨\")\nplt.axvline(max_rent, color=\"red\", linestyle=\"dashed\", linewidth=1, label=f\"Max: {max_rent:.0f}‚Ç¨\")\nplt.axvline(avg_rent, color=\"green\", linestyle=\"dashed\", linewidth=1, label=f\"Avg: {avg_rent:.1f}‚Ç¨\")\nplt.axvline(median_rent, color=\"purple\", linestyle=\"dashed\", linewidth=1, label=f\"Median: {median_rent:.1f}‚Ç¨\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#sizes",
    "href": "notebooks/flat_ads.html#sizes",
    "title": "WG Gesucht Stats",
    "section": "Sizes",
    "text": "Sizes\n\n\nCode\nsize_filtered = filter_outliers(df[\"size\"])\n\n# Calculate stats for size\nmin_size = size_filtered.min()\nmax_size = size_filtered.max()\navg_size = size_filtered.mean()\nmedian_size = size_filtered.median()\n\n# Plot histogram for sizes\nplt.hist(size_filtered, bins=10, edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Size (m¬≤)\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of Sizes ({time_min} ‚Äì {time_max}, outliers removed)\")\n\n# Add lines for stats\nplt.axvline(min_size, color=\"blue\", linestyle=\"dashed\", linewidth=1, label=f\"Min: {min_size:.0f} m¬≤\")\nplt.axvline(max_size, color=\"red\", linestyle=\"dashed\", linewidth=1, label=f\"Max: {max_size:.0f} m¬≤\")\nplt.axvline(avg_size, color=\"green\", linestyle=\"dashed\", linewidth=1, label=f\"Avg: {avg_size:.1f} m¬≤\")\nplt.axvline(median_size, color=\"purple\", linestyle=\"dashed\", linewidth=1, label=f\"Median: {median_size:.1f} m¬≤\")\n\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#inhabitants",
    "href": "notebooks/flat_ads.html#inhabitants",
    "title": "WG Gesucht Stats",
    "section": "Inhabitants",
    "text": "Inhabitants\n\n\nCode\n# Minimum inhabitants to consider\nmin_inh_allowed = 1\n\ninh_filtered = df[\"total_inhabitants\"][df[\"total_inhabitants\"] &gt;= min_inh_allowed]\n\n# Calculate stats\nmin_inh = inh_filtered.min()\nmax_inh = inh_filtered.max()\navg_inh = inh_filtered.mean()\nmedian_inh = inh_filtered.median()\n\n# Plot histogram\nplt.hist(\n    inh_filtered,\n    bins=range(int(min_inh), int(max_inh) + 2),\n    edgecolor=\"black\",\n    alpha=0.7,\n    align=\"left\",\n    rwidth=0.8\n)\nplt.xlabel(\"Number of Inhabitants\")\nplt.ylabel(\"Count\")\nplt.title(f\"Distribution of Inhabitants per WG ({time_min} ‚Äì {time_max}, min={min_inh_allowed})\")\n\n# Add lines for stats\nplt.axvline(min_inh, color=\"blue\", linestyle=\"dashed\", linewidth=1, label=f\"Min: {min_inh:.0f}\")\nplt.axvline(max_inh, color=\"red\", linestyle=\"dashed\", linewidth=1, label=f\"Max: {max_inh:.0f}\")\nplt.axvline(avg_inh, color=\"green\", linestyle=\"dashed\", linewidth=1, label=f\"Avg: {avg_inh:.1f}\")\nplt.axvline(median_inh, color=\"purple\", linestyle=\"dashed\", linewidth=1, label=f\"Median: {median_inh:.1f}\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compute average composition per ad (normalize per WG)\navg_female = (df[\"female_inhabitants\"] / df[\"total_inhabitants\"]).mean()\navg_male = (df[\"male_inhabitants\"] / df[\"total_inhabitants\"]).mean()\navg_diverse = (df[\"diverse_inhabitants\"] / df[\"total_inhabitants\"]).mean()\n\n# Data for pie chart\navg_composition = [avg_female, avg_male, avg_diverse]\nlabels = [\"Female\", \"Male\", \"Diverse\"]\ncolors = [\"pink\", \"skyblue\", \"orange\"]\n\nplt.pie(avg_composition, labels=labels, autopct=\"%.1f%%\", colors=colors, startangle=90)\nplt.title(f\"Average Inhabitant Composition per WG ({time_min} ‚Äì {time_max})\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Categorize ads\ndef wg_type(row):\n    f, m, d = row[\"female_inhabitants\"], row[\"male_inhabitants\"], row[\"diverse_inhabitants\"]\n    if f &gt; 0 and m == 0 and d == 0:\n        return \"All-female\"\n    elif m &gt; 0 and f == 0 and d == 0:\n        return \"All-male\"\n    else:\n        return \"Mixed\"\n\ndf[\"wg_type\"] = df.apply(wg_type, axis=1)\n\n# Count ads per category\nwg_counts = df[\"wg_type\"].value_counts()\n\n# Pie chart with numbers\nlabels = [f\"{cat} ({count})\" for cat, count in wg_counts.items()]\nplt.pie(wg_counts, labels=labels, autopct=\"%.1f%%\", startangle=90, colors=[\"pink\",\"skyblue\",\"lightgreen\"])\nplt.title(f\"WG Type Distribution ({time_min} ‚Äì {time_max})\")\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#districts",
    "href": "notebooks/flat_ads.html#districts",
    "title": "WG Gesucht Stats",
    "section": "Districts",
    "text": "Districts\n\n\nCode\ndistrict_counts_df = df[\"district\"].value_counts().reset_index()\ndistrict_counts_df\n\n\n\n\n\n\n\n\n\ndistrict\ncount\n\n\n\n\n0\nNeuk√∂lln\n342\n\n\n1\nPrenzlauer Berg\n314\n\n\n2\nFriedrichshain\n284\n\n\n3\nKreuzberg\n243\n\n\n4\nMitte\n222\n\n\n...\n...\n...\n\n\n202\nAlt-Tegel\n1\n\n\n203\nKaulsdorf\n1\n\n\n204\nDanzigerstr / Prenzallee\n1\n\n\n205\nRosenthal\n1\n\n\n206\nPankow/Prenzlauer Berg\n1\n\n\n\n\n207 rows √ó 2 columns\n\n\n\n\n\nCode\n# Take top 10 districts by count\ntop10_districts = district_counts_df.head(10).set_index(\"district\")[\"count\"]\n# Pie chart with counts included in labels\ntop10_labels = [f\"{district} ({count})\" for district, count in top10_districts.items()]\n\nplt.pie(\n    top10_districts,\n    labels=top10_labels,\n    autopct=\"%.1f%%\",\n    startangle=90\n)\nplt.title(f\"Top 10 Districts by Number of Ads ({time_min} ‚Äì {time_max})\")\nplt.show()"
  },
  {
    "objectID": "notebooks/flat_ads.html#spatialdistrict-analysis",
    "href": "notebooks/flat_ads.html#spatialdistrict-analysis",
    "title": "WG Gesucht Stats",
    "section": "Spatial/District Analysis",
    "text": "Spatial/District Analysis\n\n\nCode\ngdf = gpd.read_file(\"data/berlin_ortsteile.geojson\")\nprint(gdf.columns)\nprint(gdf.head())\n\n\nIndex(['gml_id', 'spatial_name', 'spatial_alias', 'spatial_type', 'OTEIL',\n       'BEZIRK', 'FLAECHE_HA', 'geometry'],\n      dtype='object')\n             gml_id spatial_name spatial_alias spatial_type         OTEIL  \\\n0  re_ortsteil.0101         0101         Mitte      Polygon         Mitte   \n1  re_ortsteil.0102         0102        Moabit      Polygon        Moabit   \n2  re_ortsteil.0103         0103  Hansaviertel      Polygon  Hansaviertel   \n3  re_ortsteil.0104         0104    Tiergarten      Polygon    Tiergarten   \n4  re_ortsteil.0105         0105       Wedding      Polygon       Wedding   \n\n  BEZIRK  FLAECHE_HA                                           geometry  \n0  Mitte   1063.8748  POLYGON ((13.41649 52.52696, 13.41635 52.52702...  \n1  Mitte    768.7909  POLYGON ((13.33884 52.51974, 13.33884 52.51974...  \n2  Mitte     52.5337  POLYGON ((13.34322 52.51557, 13.34323 52.51557...  \n3  Mitte    516.0672  POLYGON ((13.36879 52.49878, 13.36891 52.49877...  \n4  Mitte    919.9112  POLYGON ((13.34656 52.53879, 13.34664 52.53878...  \n\n\n\n\nCode\nunique_ortsteile = gdf[\"OTEIL\"].unique()\nunique_ortsteile.sort()\n\ndf_unique_ortsteile = pd.DataFrame(unique_ortsteile, columns=[\"Ortsteil\"])\ndf_unique_ortsteile\n\n\n\n\n\n\n\n\n\nOrtsteil\n\n\n\n\n0\nAdlershof\n\n\n1\nAlt-Hohensch√∂nhausen\n\n\n2\nAlt-Treptow\n\n\n3\nAltglienicke\n\n\n4\nBaumschulenweg\n\n\n...\n...\n\n\n91\nWilhelmsruh\n\n\n92\nWilhelmstadt\n\n\n93\nWilmersdorf\n\n\n94\nWittenau\n\n\n95\nZehlendorf\n\n\n\n\n96 rows √ó 1 columns\n\n\n\n\n\nCode\n# Just the non-specific \"Berlin\" Ortsteil is not included ofc\ntop_districts = set(district_counts_df.head(20)[\"district\"].tolist())\nnot_in_ortsteile = top_districts - set(unique_ortsteile.tolist())\nnot_in_ortsteile\n\n\n{'Berlin',\n 'Charlottenburg-Wilmersdorf',\n 'Friedrichshain-Kreuzberg',\n 'Tempelhof-Sch√∂neberg'}\n\n\n\n\nCode\ndistricts_to_visualize = top_districts - not_in_ortsteile\ndistricts_to_visualize\n\n\n{'Charlottenburg',\n 'Friedrichshain',\n 'Kreuzberg',\n 'Lichtenberg',\n 'Mitte',\n 'Moabit',\n 'Neuk√∂lln',\n 'Pankow',\n 'Prenzlauer Berg',\n 'Reinickendorf',\n 'Sch√∂neberg',\n 'Spandau',\n 'Steglitz',\n 'Tempelhof',\n 'Wedding',\n 'Wilmersdorf'}\n\n\n\n\nCode\n# keep only top-x to show\ntop10 = district_counts_df[district_counts_df[\"district\"].isin(districts_to_visualize)]\n\n# merge counts onto geometries\ngdf_top = gdf.merge(top10, left_on=\"OTEIL\", right_on=\"district\", how=\"inner\")\n\n# nice, consistent light‚Üístrong coloring\nvmin, vmax = gdf_top[\"count\"].min(), gdf_top[\"count\"].max()\n\n# background\nax = gdf.plot(edgecolor=\"black\", linewidth=0.4, alpha=0.05, figsize=(8, 8))\n\n# choropleth (no legend), consistent cmap range\ngdf_top.plot(\n    ax=ax,\n    column=\"count\",\n    cmap=\"Oranges\",\n    vmin=vmin,\n    vmax=vmax,\n    edgecolor=\"black\",\n    linewidth=0.8,\n    alpha=0.9,\n)\n\n# labels (count + name) with white stroke for readability\nfor _, r in gdf_top.iterrows():\n    x, y = r.geometry.representative_point().coords[0]\n    ax.text(\n        x, y, f\"{int(r['count'])}\\n{r['OTEIL']}\",\n        ha=\"center\", va=\"center\", fontsize=9, linespacing=0.9,\n        path_effects=[pe.withStroke(linewidth=2.5, foreground=\"white\")]\n    )\n\n# zoom to selected\nxmin, ymin, xmax, ymax = gdf_top.total_bounds\nax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n\nax.set_title(f\"Top Districts by Ad Count ({time_min} ‚Äì {time_max})\")\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# avg size per district\navg_size_df = (\n    df.groupby(\"district\", as_index=False)[\"size\"]\n      .median()\n      .rename(columns={\"size\": \"avg_sqm\"})\n)\n\n# keep only top-10 to show\ntop10_avg = avg_size_df[avg_size_df[\"district\"].isin(districts_to_visualize)]\n\n# merge onto geometries\ngdf_avg = gdf.merge(top10_avg, left_on=\"OTEIL\", right_on=\"district\", how=\"inner\")\n\n# consistent light‚Üístrong color scale\nvmin, vmax = gdf_avg[\"avg_sqm\"].min(), gdf_avg[\"avg_sqm\"].max()\n\n# background\nax = gdf.plot(edgecolor=\"black\", linewidth=0.4, alpha=0.05, figsize=(8, 8))\n\n# choropleth (no legend)\ngdf_avg.plot(\n    ax=ax,\n    column=\"avg_sqm\",\n    cmap=\"Blues\",\n    vmin=vmin,\n    vmax=vmax,\n    edgecolor=\"black\",\n    linewidth=0.8,\n    alpha=0.9,\n)\n\n# labels: avg m¬≤ + name\nfor _, r in gdf_avg.iterrows():\n    x, y = r.geometry.representative_point().coords[0]\n    ax.text(\n        x, y, f\"{r['avg_sqm']:.1f} m¬≤\\n{r['OTEIL']}\",\n        ha=\"center\", va=\"center\", fontsize=9, linespacing=0.9,\n        path_effects=[pe.withStroke(linewidth=2.5, foreground=\"white\")]\n    )\n\n# zoom to selected\nxmin, ymin, xmax, ymax = gdf_avg.total_bounds\nax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n\nax.set_title(\"Top Districts by Median Room Size (m¬≤)\")\nax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\navg_rent_df = (\n    df.groupby(\"district\", as_index=False)[\"rent\"]\n      .median()\n      .rename(columns={\"rent\": \"avg_rent\"})\n)\n\ntop10_avg_rent = avg_rent_df[avg_rent_df[\"district\"].isin(districts_to_visualize)]\ngdf_avg_rent = gdf.merge(top10_avg_rent, left_on=\"OTEIL\", right_on=\"district\", how=\"inner\")\n\nvmin, vmax = gdf_avg_rent[\"avg_rent\"].min(), gdf_avg_rent[\"avg_rent\"].max()\n\nax = gdf.plot(edgecolor=\"black\", linewidth=0.4, alpha=0.05, figsize=(8, 8))\ngdf_avg_rent.plot(\n    ax=ax, column=\"avg_rent\", cmap=\"Reds\", vmin=vmin, vmax=vmax,\n    edgecolor=\"black\", linewidth=0.8, alpha=0.9\n)\n\nfor _, r in gdf_avg_rent.iterrows():\n    x, y = r.geometry.representative_point().coords[0]\n    ax.text(\n        x, y, f\"{r['avg_rent']:.0f}‚Ç¨\\n{r['OTEIL']}\",\n        ha=\"center\", va=\"center\", fontsize=9, linespacing=0.9,\n        path_effects=[pe.withStroke(linewidth=2.5, foreground=\"white\")]\n    )\n\nxmin, ymin, xmax, ymax = gdf_avg_rent.total_bounds\nax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\nax.set_title(\"Top Districts by Median Rent (‚Ç¨)\")\nax.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Welcome to WG Gesucht Stats!",
    "section": "",
    "text": "Use the navigation bar to open the data analyses."
  }
]